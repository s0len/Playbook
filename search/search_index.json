{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Playbook Documentation","text":"<p>Sonarr for Sports \u2013 Automated file matching, renaming, and metadata for sports content in Plex.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>Love watching sports replays in Plex but hate manually renaming files and setting metadata? Traditional tools like Sonarr don't work for sports because there's no centralized database like TheTVDB. Every sport structures their seasons differently (F1 has races, UFC has events, NFL has weeks), and release groups use wildly inconsistent naming schemes.</p>"},{"location":"#how-playbook-solves-it","title":"How Playbook Solves It","text":"<p>Playbook is a complete pipeline that bridges the gap between messy downloads and perfectly organized Plex libraries:</p>"},{"location":"#1-the-database-layer","title":"1. The Database Layer","text":"<p>Custom scrapers pull sports schedules from various sources (SportsDB, official APIs, manual curation) and structure them as YAML files that mirror how Plex expects TV shows: Show \u2192 Season \u2192 Episode. This is the foundation \u2013 every sport gets its own \"TVDb\" equivalent.</p>"},{"location":"#2-smart-file-matching-like-sonarr","title":"2. Smart File Matching (like Sonarr)","text":"<p>Playbook scans your downloads, parses filenames using regex patterns (built-in packs for F1, MotoGP, UFC, NFL, NBA, NHL, etc.), matches them against the YAML database, and automatically renames/moves them to your Plex library with perfect naming.</p>"},{"location":"#3-rich-metadata-via-kometa","title":"3. Rich Metadata (via Kometa)","text":"<p>The same YAML files that power matching also feed Kometa to set posters, summaries, air dates, and episode titles. One source of truth for everything.</p>"},{"location":"#why-its-a-game-changer","title":"Why It's a Game-Changer","text":"<ul> <li>One YAML file does it all: episode matching + metadata + Kometa integration</li> <li>Declarative: Swap leagues, change folder structures, or add new release groups without touching Python</li> <li>Complete automation: From download to Plex-ready with proper artwork and descriptions</li> <li>Built for sports: Handles special cases like sprint races, prelims, qualifying sessions, and multi-part events</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>All sports are enabled by default! Just configure your directories:</p> <ol> <li>Copy <code>config/playbook.sample.yaml</code> to <code>playbook.yaml</code></li> <li>Set <code>SOURCE_DIR</code> (where downloads land) and <code>DESTINATION_DIR</code> (Plex library)</li> <li>Run a dry-run to test: <code>playbook process --dry-run</code></li> <li>Let it run automatically with <code>playbook watch</code> or Docker</li> </ol> <p>That's it! Playbook automatically processes Formula 1, MotoGP, UFC, NFL, NBA, NHL, Premier League, Champions League, and more. Use <code>disabled_sports</code> to exclude any sports you don't want.</p> <p>See Getting Started for detailed installation (Docker, Python, Kubernetes) or run <code>make docs-serve</code> for local docs at <code>http://127.0.0.1:8000</code>.</p>"},{"location":"#documentation-map","title":"Documentation Map","text":"<ul> <li>Getting Started \u2013 installation, environment setup, first-run checklist</li> <li>Configuration Guide \u2013 YAML schema, patterns, notifications, templating</li> <li>Operations &amp; Run Modes \u2013 CLI commands, watcher mode, logging, upgrades</li> <li>Integrations \u2013 Kometa triggers, Autobrr filters, Autoscan, Plex setup</li> <li>Recipes &amp; How-tos \u2013 sport-specific walkthroughs, custom patterns, real examples</li> <li>Troubleshooting &amp; FAQ \u2013 diagnostics, cache resets, common issues</li> <li>Developer Guide \u2013 contributing, testing, release workflow</li> <li>Changelog \u2013 release notes and version history</li> </ul> <p>Missing something? Use the search box or open an issue with your question.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/","title":"Playbook Matching Improvements Plan","text":"<p>This document outlines the changes needed to achieve 100% match rate for the ~600 currently unmatched files.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#implementation-status","title":"Implementation Status","text":"Category Files Status Notes NHL ISO date format ~30+ DONE Added <code>NHL-*</code> and <code>nhl-*</code> source globs UFC On ESPN ~20+ DONE Added patterns and source globs F1 Pre/Post Show ~15+ DONE Fixed regex for hyphen + <code>.Show</code> suffix F1 Weekend Warm-Up ~5+ DONE Fixed regex for hyphen variant IndyCar Round format ~10+ DONE Changed <code>\\d{2}</code> to <code>\\d{1,2}</code> Figure Skating location ~5+ DONE Added location-to-event-name aliases MotoGP 2026 Future DONE Added 2026 variant UFC season matching 17 DONE Changed from <code>mode: title</code> to <code>mode: round</code> WTA/Tennis files ~400+ Low Priority Complex - many formats excluded by globs NFL variant format ~50+ Pending May need additional patterns"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#completed-fixes","title":"Completed Fixes","text":""},{"location":"MATCHING_IMPROVEMENTS_PLAN/#1-nhl-iso-date-format-source-globs","title":"1. NHL - ISO date format source globs","text":"<p>Added <code>NHL-*</code> and <code>nhl-*</code> to default_source_globs to match files like <code>NHL-2025-11-22_NJD@PHI.mkv</code>.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#2-ufc-on-espn-new-patterns-and-source-globs","title":"2. UFC On ESPN - New patterns and source globs","text":"<p>Added 4 new patterns for UFC On ESPN events: - Dot separated with session tag - Space separated with session tag - Dot separated without session tag - Space separated without session tag</p> <p>Added source globs: <code>UFC.On.ESPN.*</code>, <code>UFC On ESPN *</code>, <code>ufc.on.espn.*</code>, <code>ufc on espn *</code></p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#3-ufc-season-matching-changed-to-round-based","title":"3. UFC Season Matching - Changed to round-based","text":"<p>Changed all UFC patterns from <code>mode: title</code> to <code>mode: round</code> with <code>group: season</code>. This matches UFC events by their event number (321, 263, 73, etc.) against the metadata's <code>round_number</code> or <code>display_number</code>, which is more reliable than title string matching.</p> <p>Why this is better: - Title matching fails when filenames have spelling variations (e.g., <code>makhacheve</code> vs <code>makhachev</code>) - Event numbers are consistent between filenames and metadata - No need for complex title normalization logic</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#4-formula-1-prepost-show-hyphen-and-show-suffix","title":"4. Formula 1 - Pre/Post Show hyphen and .Show suffix","text":"<p>Updated regex to handle: - Hyphen separator: <code>Pre-Race</code> instead of <code>Pre.Race</code> - <code>.Show</code> suffix: <code>Post-Sprint.Show</code> - Weekend Warm-Up hyphen variant</p> <p>Added session aliases for all hyphen variants.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#5-indycar-round-number-without-leading-zero","title":"5. IndyCar - Round number without leading zero","text":"<p>Changed <code>Round(?P&lt;round&gt;\\d{2})</code> to <code>Round(?P&lt;round&gt;\\d{1,2})</code> to match both <code>Round06</code> and <code>Round6</code>.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#6-figure-skating-location-to-event-name-aliases","title":"6. Figure Skating - Location to event name aliases","text":"<p>Added aliases to map captured location names to actual ISU Grand Prix event names: - <code>United State</code> / <code>United States</code> / <code>USA</code> / <code>US</code> \u2192 <code>Skate America</code> - <code>France</code> \u2192 <code>Internationaux de France</code> - <code>Canada</code> \u2192 <code>Skate Canada International</code> - <code>Japan</code> \u2192 <code>NHK Trophy</code> - <code>China</code> \u2192 <code>Cup of China</code> - <code>Finland</code> \u2192 <code>Grand Prix of Espoo</code> - <code>Final</code> \u2192 <code>Grand Prix Final</code></p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#7-motogp-2026-variant","title":"7. MotoGP 2026 variant","text":"<p>Added 2026 variant with <code>show_slug: \"motogp-2026\"</code>.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#8-wta-united-cup-source-globs","title":"8. WTA United Cup source globs","text":"<p>Added <code>*United Cup*</code> and <code>*United.Cup*</code> to WTA source globs.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#test-commands","title":"Test Commands","text":"<pre><code># Run pattern sample tests\npytest tests/test_pattern_samples.py -v\n\n# Run matcher tests\npytest tests/test_matcher.py -v\n\n# Run with trace output to debug specific files\npython -m playbook.cli --config config/playbook.yaml --dry-run --verbose --trace-matches\n</code></pre>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#files-modified","title":"Files Modified","text":"<ol> <li><code>src/playbook/pattern_templates.yaml</code> - All pattern changes</li> <li><code>tests/data/pattern_samples.yaml</code> - Updated UFC test data to use event numbers as round_numbers</li> </ol>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#remaining-work","title":"Remaining Work","text":""},{"location":"MATCHING_IMPROVEMENTS_PLAN/#nfl-variant-format-if-needed","title":"NFL variant format (if needed)","text":"<p>Files like <code>NFL 07-12-2025 Week14 Houston Texans vs...</code> may need additional pattern work if current patterns don't match.</p>"},{"location":"MATCHING_IMPROVEMENTS_PLAN/#wtatennis-files","title":"WTA/Tennis files","text":"<p>Complex and highly variable naming. The <code>allow_unmatched: true</code> flag is set to suppress warnings for exotic formats.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>The canonical changelog lives at <code>CHANGELOG.md</code>. Keeping it at the repo root makes it visible on GitHub releases while this page surfaces it in the docs sidebar.</p> <p>Latest releases:</p> <ul> <li>View <code>CHANGELOG.md</code> on GitHub</li> <li>Grab historical tags from the releases page</li> </ul> <p>Tip: when landing a feature, update both <code>CHANGELOG.md</code> and the relevant <code>docs/</code> page so releases stay traceable.</p>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Every Playbook deployment runs from a single YAML file. Start by copying <code>config/playbook.sample.yaml</code>, then enable sports, pattern sets, and integrations one section at a time. This guide mirrors the dataclasses under <code>src/playbook/config.py</code>.</p>"},{"location":"configuration/#1-global-settings","title":"1. Global Settings","text":"Field Description Default <code>source_dir</code> Root directory containing downloads to normalize. <code>/data/source</code> <code>destination_dir</code> Library root where organized folders/files are created. <code>/data/destination</code> <code>cache_dir</code> Metadata cache directory (<code>metadata/&lt;hash&gt;.json</code>). Safe to delete to force refetch. <code>/data/cache</code> <code>dry_run</code> When <code>true</code>, logs intent but skips filesystem writes. <code>false</code> <code>skip_existing</code> Leave destination files untouched unless a higher-priority release arrives. <code>true</code> <code>link_mode</code> Default link behavior: <code>hardlink</code>, <code>copy</code>, or <code>symlink</code>. <code>hardlink</code> <code>use_default_sports</code> Auto-enable all built-in sports. Set <code>false</code> to only use explicitly defined sports. <code>true</code> <code>disabled_sports</code> List of sport base IDs to exclude from defaults (e.g., <code>[formula_e, moto2]</code>). <code>[]</code> <code>file_watcher.enabled</code> Keeps Playbook running and reacts to filesystem events. <code>false</code> <code>file_watcher.paths</code> Directories to observe; defaults to <code>source_dir</code> when empty. <code>[]</code> <code>file_watcher.include</code> / <code>ignore</code> Glob filters to allow/skip events (e.g., ignore <code>*.part</code>). <code>[]</code> / <code>[\"*.part\",\"*.tmp\"]</code> <code>file_watcher.debounce_seconds</code> Minimum seconds between watcher-triggered runs. <code>5</code> <code>file_watcher.reconcile_interval</code> Forces a full scan every N seconds even if no events arrive. <code>900</code> <code>destination.*</code> Default templates for root folder, season folder, and filename. See sample"},{"location":"configuration/#default-sports","title":"Default Sports","text":"<p>When <code>use_default_sports: true</code> (the default), Playbook automatically enables these sports:</p> Category Sports Motorsport Formula 1, Formula E, IndyCar, MotoGP, Moto2, Moto3, World Superbike, World Supersport, Isle of Man TT Combat Sports UFC North American NFL, NBA, NHL Football (Soccer) Premier League, UEFA Champions League Figure Skating ISU Figure Skating <p>To disable specific sports:</p> <pre><code>settings:\n  disabled_sports:\n    - formula_e      # Disable Formula E\n    - moto2          # Disable Moto2\n    - moto3          # Disable Moto3\n</code></pre> <p>To disable all defaults and only use your explicit sports:</p> <pre><code>settings:\n  use_default_sports: false\n\nsports:\n  - id: formula1\n    # ... your custom config\n</code></pre>"},{"location":"configuration/#notifications-autoscan","title":"Notifications &amp; Autoscan","text":"<p>Define Discord/Slack/webhook/email/autoscan targets under <code>notifications.targets</code>. Entries can override mention mappings defined globally:</p> <pre><code>notifications:\n  mentions:\n    premier_league: \"&lt;@&amp;123456789012345678&gt;\"\n    formula1: \"&lt;@&amp;222333444555666777&gt;\"\n    default: \"@everyone\"\n  targets:\n    - type: discord\n      webhook_env: DISCORD_WEBHOOK_URL\n      # webhook_url: ${DISCORD_WEBHOOK_URL}  # Optional inline expansion if your config templating supports it.\n      mentions:\n        formula1: \"&lt;@&amp;999&gt;\"\n    - type: autoscan\n      url: http://autoscan:3030\n      trigger: manual\n      rewrite:\n        - from: ${DESTINATION_DIR:-/data/destination}\n          to: /mnt/unionfs/Media\n</code></pre> <p>Set <code>webhook_env</code> to the name of an environment variable (in your container/host manifest) to keep secrets out of the YAML file. Playbook resolves the value at runtime and skips the target if the variable is absent. If you already template the config file yourself you can keep using <code>webhook_url</code> with <code>${VAR}</code> syntax; both approaches continue to work.</p> <p>Autoscan entries mirror the manual trigger. Rewrites translate container paths to Plex-visible mount points. Every successful <code>new</code>/<code>changed</code> event sends the parent directory of the destination file.</p> <p>Supported target types</p> <ul> <li><code>discord</code> \u2013 rich embeds with optional mention overrides per webhook.</li> <li><code>slack</code> \u2013 simple JSON payload; add your preferred text template.</li> <li><code>webhook</code> \u2013 fully templatable JSON for custom receivers.</li> <li><code>email</code> \u2013 SMTP transport with subject/body templates.</li> <li><code>autoscan</code> \u2013 immediately ping Autoscan so Plex/Jellyfin rescans the destination directories.</li> </ul> <p>Batching knobs (<code>notifications.batch_daily</code>, <code>notifications.flush_time</code>) apply to any target that supports rolling updates (Discord embeds today). Use <code>notifications.mentions</code> to fan out role/user mentions by sport ID (supports shell-style globs), and override them per target when one destination needs custom pings.</p>"},{"location":"configuration/#autoscan-example","title":"Autoscan example","text":"<p><code>--8&lt;-- \"snippets/notifications-autoscan.md\"</code></p> <p>Define multiple <code>rewrite</code> map entries when Autoscan lives in another container or when Plex views different mount points.</p>"},{"location":"configuration/#file-watcher-settings","title":"File Watcher Settings","text":"<p>The watcher keeps Playbook alive and reacts to filesystem events instead of relying solely on periodic scans:</p> <ul> <li><code>file_watcher.paths</code> defaults to <code>source_dir</code>. List additional absolute or relative paths to watch multiple download roots.</li> <li><code>include</code> / <code>ignore</code> accept glob syntax. Common ignores: <code>[\"*.part\", \"*.tmp\", \"*.!qb\"]</code>.</li> <li><code>debounce_seconds</code> batches bursts of events into a single processor run. Increase it when downloaders generate rapid file-change storms.</li> <li><code>reconcile_interval</code> performs a full scan every N seconds even if the platform drops events.</li> <li>Override <code>WATCH_MODE=true|false</code> (or use <code>--watch</code> / <code>--no-watch</code>) to force the CLI into the desired mode regardless of the config.</li> </ul>"},{"location":"configuration/#2-tvsportsdb-api-configuration","title":"2. TVSportsDB API Configuration","text":"<p>Playbook fetches show/season/episode metadata from TVSportsDB REST API. The defaults work for most users, but you can optionally tune cache and timeout settings under <code>settings.tvsportsdb</code>:</p> <pre><code>settings:\n  tvsportsdb:\n    ttl_hours: 6     # Cache API responses for 6 hours (default: 12)\n    timeout: 60      # HTTP request timeout in seconds (default: 30)\n</code></pre> Field Description Default <code>ttl_hours</code> How long to cache API responses before refreshing <code>12</code> <code>timeout</code> HTTP request timeout in seconds <code>30</code> <p>This section is optional\u2014omit it entirely to use defaults.</p>"},{"location":"configuration/#3-sport-entries","title":"3. Sport Entries","text":"<p>Most users don't need this section</p> <p>All supported sports are enabled by default with correct configurations. Only define sports here if you need to:</p> <ul> <li>Override settings for a specific sport (e.g., custom quality profile)</li> <li>Add a custom sport not in the defaults</li> <li>Replace a default sport with different variants/slugs</li> </ul> <p>Each sport links to a show in TVSportsDB via <code>show_slug</code> and defines detection filters and pattern packs:</p> <pre><code>- id: formula1_2025\n  name: Formula 1 2025\n  enabled: true\n  show_slug: \"formula-1-2025\"    # TVSportsDB show slug (REQUIRED)\n  source_globs:\n    - \"Formula.1.*\"\n  source_extensions:\n    - .mkv\n    - .mp4\n  allow_unmatched: false\n  pattern_sets:\n    - formula1\n  season_overrides:\n    Pre-Season Testing:\n      season_number: 0\n      round: 0\n</code></pre> <p>Key knobs:</p> <ul> <li><code>show_slug</code> (required) references the show in TVSportsDB. Find available slugs at the API endpoint or TVSportsDB web interface.</li> <li><code>enabled</code> toggles sports without deleting them.</li> <li><code>source_globs</code> / <code>source_extensions</code> are coarse filters before any regex work happens.</li> <li><code>link_mode</code>, <code>destination.*</code>, and notification overrides let you specialize behavior per sport.</li> </ul> <p>Supported file extensions</p> <p>By default, Playbook processes files with these extensions: <code>.mkv</code>, <code>.mp4</code>, <code>.ts</code>, <code>.m4v</code>, <code>.avi</code>. The <code>.ts</code> extension supports MPEG Transport Stream files commonly used in TV recordings.</p> <p>Important: When you set <code>source_extensions</code> on a sport, it replaces the defaults entirely\u2014it does not merge with them. If you override this field, include all extensions you want to match:</p> <p><pre><code>source_extensions:\n  - .mkv\n  - .mp4\n  - .ts   # Don't forget .ts if you have transport stream files!\n  - .m4v\n  - .avi\n</code></pre> - <code>team_alias_map</code> (optional) points to a built-in alias table (e.g., <code>premier_league</code>, <code>nhl</code>) used by the structured matcher to normalize shorthand like \"Man City\" or \"NJD\". - <code>pattern_sets</code> pulls from <code>src/playbook/pattern_templates.yaml</code>; you can still inline <code>file_patterns</code> for overrides.</p> <p>Metadata overrides</p> <ul> <li><code>season_overrides</code> lets you force numbers/titles for special events (exhibitions, pre-season testing, etc.). Define these at the sport level.</li> <li><code>tvsportsdb.ttl_hours</code> controls how aggressively Playbook refreshes API responses; lower it for rapidly changing leagues.</li> <li><code>allow_unmatched: true</code> is useful when onboarding a new sport\u2014you'll still see matches in the log but without warnings.</li> </ul>"},{"location":"configuration/#pattern-sets-reuse","title":"Pattern Sets &amp; Reuse","text":"<p>Pattern sets bundle curated regex/alias definitions under <code>src/playbook/pattern_templates.yaml</code>. To inspect them:</p> <pre><code>rg --context 3 \"pattern_sets\" src/playbook/pattern_templates.yaml\n</code></pre> <ul> <li>Reference a bundle via <code>pattern_sets: [\"formula1\", \"motoGP\"]</code>.</li> <li>Layer sport-specific overrides by combining <code>pattern_sets</code> with inline <code>file_patterns</code>.</li> <li>Keep experimental tweaks local until they're stable, then upstream them by editing <code>pattern_templates.yaml</code>.</li> </ul>"},{"location":"configuration/#4-pattern-matching","title":"4. Pattern Matching","text":"<p>Inline patterns support full regex/group control:</p> <pre><code>file_patterns:\n  - regex: \"(?i)^Formula\\\\.1\\\\.(?P&lt;year&gt;\\\\d{4})\\\\.Round(?P&lt;round&gt;\\\\d{2})\\\\.(?P&lt;location&gt;[^.]+)\\\\.(?P&lt;session&gt;[^.]+)\"\n    description: Canonical multi-session weekend releases\n    priority: 10\n    season_selector:\n      mode: round\n      group: round\n    episode_selector:\n      group: session\n    session_aliases:\n      Race: [\"Race\"]\n      Sprint: [\"Sprint.Race\", \"Sprint\"]\n      Qualifying: [\"Qualifying\", \"Quali\"]\n      Free Practice 1: [\"FP1\", \"Free.Practice.1\"]\n</code></pre> <p>Reference table:</p> <ul> <li><code>regex</code> must expose the capture groups consumed by selectors/templates.</li> <li><code>season_selector</code> supports <code>round</code>, <code>key</code>, <code>title</code>, <code>sequential</code>, and <code>date</code> modes plus offsets/mappings.</li> <li><code>episode_selector</code> chooses which capture identifies an episode; set <code>allow_fallback_to_title</code> when a regex omits the session.</li> <li><code>session_aliases</code> augment metadata aliases with release-specific tokens (case-insensitive).</li> <li><code>priority</code> resolves collisions when multiple patterns match the same file.</li> </ul> <p><code>value_template</code> lets a selector compose a derived lookup key from multiple capture groups (e.g., <code>{date_year}-{month:0&gt;2}-{day:0&gt;2}</code> to normalize <code>10 11</code> \u2192 <code>2025-11-10</code>). The <code>date</code> selector mode uses the formatted value to locate the season containing an episode with the same <code>originally_available</code> date\u2014perfect for leagues scheduled by calendar days rather than round numbers.</p> <p>Testing patterns quickly</p> <ol> <li>Drop real release names into <code>tests/data/pattern_samples.yaml</code>.</li> <li>Run <code>pytest tests/test_pattern_samples.py</code> to confirm they map to the expected metadata.</li> <li>Use <code>python -m playbook.cli --config playbook.yaml --dry-run --verbose</code> with <code>--clear-processed-cache</code> to reprocess the same files repeatedly during tuning.</li> </ol>"},{"location":"configuration/#5-destination-templating","title":"5. Destination Templating","text":"<p>Templates receive a rich context assembled from metadata + regex captures:</p> Key Meaning <code>sport_id</code>, <code>sport_name</code> Sport metadata from the config. <code>show_title</code>, <code>show_slug</code> Display title and API slug from TVSportsDB. <code>season_title</code>, <code>season_number</code>, <code>season_round</code>, <code>season_year</code> Season context with overrides applied. <code>episode_title</code>, <code>episode_number</code>, <code>episode_summary</code>, <code>episode_originally_available</code> Episode metadata pulled from the feed. <code>location</code>, <code>session</code>, <code>round</code>, \u2026 Any capture group from the regex. <code>source_filename</code>, <code>extension</code>, <code>relative_source</code> Safe access to the original file path. <p>Set per-sport or per-pattern overrides (<code>destination.root_template</code>, <code>destination.filename_template</code>) whenever a league needs special formatting.</p> <p>Tips:</p> <ul> <li>Append <code>{suffix}</code> to preserve any release-group decorations that survived sanitization.</li> <li>Use conditionals in templates (Jinja syntax) for optional segments, e.g., <code>{% if episode_originally_available %}...{% endif %}</code>.</li> <li>The renderer automatically normalizes whitespace and strips forbidden path characters\u2014keep templates readable and let Playbook worry about sanitization.</li> <li>Override <code>destination.season_folder_template</code> to group rounds (e.g., <code>{season_round:02} {season_title}</code> for touring series).</li> </ul>"},{"location":"configuration/#6-variants-reuse","title":"6. Variants &amp; Reuse","text":"<p>Reuse a base sport definition across seasons or release groups:</p> <pre><code>- id: indycar\n  name: IndyCar\n  pattern_sets:\n    - indycar\n  source_globs:\n    - \"IndyCar.*\"\n  variants:\n    - year: 2024\n      show_slug: \"ntt-indycar-series-2024\"\n    - year: 2025\n      show_slug: \"ntt-indycar-series-2025\"\n    - year: 2026\n      show_slug: \"ntt-indycar-series-2026\"\n</code></pre> <p>Each variant inherits fields from the parent, tweaks whatever is listed inside the variant block, and gets an auto-generated <code>id</code>/<code>name</code> when not explicitly set. Use variants when only the <code>show_slug</code> or alias table changes year-to-year.</p>"},{"location":"configuration/#validation-workflow","title":"Validation Workflow","text":"<ol> <li>Run <code>python -m playbook.cli validate-config --config /config/playbook.yaml --diff-sample</code> to catch schema errors early.</li> <li>Add <code>--show-trace</code> if you need full Python tracebacks.</li> <li>Commit sample configs to version control and watch the diff returned by the validator to understand exactly what deviated from <code>playbook.sample.yaml</code>.</li> </ol> <p>Ready to operate the organizer? Continue to Operations &amp; Run Modes.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>Welcome, contributor! This guide captures the repo setup, test workflow, branching policy, and release cadence.</p>"},{"location":"developer-guide/#local-environment","title":"Local Environment","text":"<pre><code>git clone https://github.com/s0len/playbook.git\ncd playbook\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npip install -r requirements-dev.txt\n</code></pre> <p>Run the CLI locally:</p> <pre><code>python -m playbook.cli --config config/playbook.sample.yaml --dry-run --verbose\n</code></pre> <p>Build the container image:</p> <pre><code>docker build -t playbook:dev .\n</code></pre>"},{"location":"developer-guide/#tests-tooling","title":"Tests &amp; Tooling","text":"<ul> <li>Primary suite: <code>pytest</code> (run via <code>pytest</code> or <code>python -m pytest</code>).  </li> <li>Pattern samples: edit <code>tests/data/pattern_samples.yaml</code> and run <code>pytest tests/test_pattern_samples.py</code>.  </li> <li>Bootstrap helper: <code>bash scripts/bootstrap_and_test.sh</code> spins up a clean virtualenv and runs the full suite.</li> <li>Formatting/linting: follow <code>ruff</code>/<code>black</code> defaults (coming soon to CI). Use <code>ruff check .</code> and <code>black .</code> before committing when touching Python files.</li> <li>Match traces: <code>python -m playbook.cli --dry-run --verbose --trace-matches</code> writes JSON artifacts in <code>cache_dir/traces</code> and is invaluable when reviewing PRs that tweak regex logic.</li> </ul>"},{"location":"developer-guide/#documentation-workflow","title":"Documentation Workflow","text":"<ul> <li>Install <code>mkdocs-material</code> via <code>pip install -r requirements-dev.txt</code>.</li> <li><code>make docs-serve</code> starts the live preview at <code>http://127.0.0.1:8000</code>.</li> <li><code>make docs-build</code> produces the static site under <code>site/</code>.</li> <li><code>make docs-deploy</code> runs <code>mkdocs gh-deploy</code> (maintainers only).</li> <li>GitHub Actions (<code>.github/workflows/docs.yml</code>) build the docs on every PR and publish to GitHub Pages when <code>main</code> changes.</li> <li>Keep reusable snippets under <code>docs/snippets/</code> and include them via <code>```--8&lt;-- \"snippets/foo.md\"```</code> so updates propagate automatically.</li> </ul>"},{"location":"developer-guide/#branching-releases","title":"Branching &amp; Releases","text":"<ul> <li><code>develop</code> collects day-to-day feature work.  </li> <li><code>main</code> always reflects the latest tagged release.  </li> <li>Feature branches follow <code>feature/&lt;area&gt;-&lt;short-description&gt;</code> (e.g., <code>feature/docs-foundation</code>, <code>feature/kometa-webhook</code>).  </li> <li>Open bite-sized PRs; large doc pushes can be split per section (config guide, integrations, recipes, etc.).  </li> <li>Release checklist:\\n  1. Ensure docs for new features merged into <code>develop</code>.\\n  2. Bump version + changelog.\\n  3. Merge <code>develop</code> \u2192 <code>main</code> via PR.\\n  4. Tag (e.g., <code>v1.4.0</code>).\\n  5. Let the docs workflow deploy GitHub Pages; update badges/links if needed.</li> <li>Automated tests run in CI via GitHub Actions; keep PRs green by running <code>pytest</code> locally before pushing.</li> <li>For doc-heavy efforts, follow the A/B/C plan (foundation \u2192 README \u2192 chapter-focused branches) so reviewers aren\u2019t overwhelmed.</li> </ul>"},{"location":"developer-guide/#contribution-etiquette","title":"Contribution Etiquette","text":"<ul> <li>Include sample configs or tests whenever you touch matching logic.\\n- Redact secrets before sharing logs/configs in issues.\\n- Use draft PRs early\u2014pattern reviews benefit from real-world filenames.\\n- Mention whether docs updates are included (there\u2019s a \u201cDocs updated?\u201d checkbox in the PR template).\\n- For architecture proposals or metadata feed requests, start a GitHub Discussion before coding.</li> </ul> <p>Need more details? Ping <code>@s0len</code> in issues/discussions or hop into the project chat.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through installing Playbook, configuring your first sport, and verifying everything works before letting it run automatically. You can deploy via Docker (recommended), Python virtual environment, or Kubernetes.</p>"},{"location":"getting-started/#quick-overview","title":"Quick Overview","text":"<p>All sports are enabled by default! Every deployment needs just three things:</p> <ol> <li>A config file (<code>playbook.yaml</code>) with your directory paths</li> <li>Three directories: source (downloads), destination (Plex library), and cache (metadata storage)</li> <li>Network access to TVSportsDB API (metadata source for show/season/episode information)</li> </ol> <p>Playbook automatically processes Formula 1, MotoGP, UFC, NFL, NBA, NHL, Premier League, Champions League, Figure Skating, and more. No need to configure individual sports unless you want to customize or disable them.</p> <p>All deployment methods use the same config schema and produce identical folder layouts in your Plex library.</p>"},{"location":"getting-started/#before-you-start","title":"Before You Start","text":"<p>Before installing, gather this information:</p> <ul> <li>Source directory \u2013 where your torrent client/downloader saves files (e.g., <code>/downloads/sport</code>)</li> <li>Destination directory \u2013 your Plex library path (e.g., <code>/library/sport</code>)</li> <li>Cache directory \u2013 persistent storage for metadata and state (e.g., <code>/cache/playbook</code>)</li> </ul> <p>Then:</p> <ol> <li>Copy <code>config/playbook.sample.yaml</code> to your preferred location (Docker defaults to <code>/config/playbook.yaml</code>)</li> <li>Edit the config to set <code>SOURCE_DIR</code>, <code>DESTINATION_DIR</code>, and <code>CACHE_DIR</code> (or provide them as environment variables)</li> <li>(Optional) Disable sports you don't want using <code>disabled_sports: [sport_id, ...]</code></li> <li>Validate your config: <code>playbook validate-config --config playbook.yaml --diff-sample</code></li> </ol> <p>Minimal Configuration</p> <p>With default sports enabled, your config can be as simple as: <pre><code>settings:\n  source_dir: /downloads/sport\n  destination_dir: /library/sport\n  cache_dir: /cache/playbook\n</code></pre> All 16 sports (26 variants) will be processed automatically!</p>"},{"location":"getting-started/#installation-methods","title":"Installation Methods","text":"<p>Choose the deployment method that fits your setup. All methods use the same config file and produce identical results.</p>"},{"location":"getting-started/#option-a-docker-recommended","title":"Option A: Docker (Recommended)","text":""},{"location":"getting-started/#step-1-test-with-a-dry-run","title":"Step 1: Test with a dry-run","text":"<p>Before running continuously, validate your setup with a one-shot dry-run:</p> <pre><code>docker run --rm -it \\\n  -e DRY_RUN=true \\\n  -e VERBOSE=true \\\n  -e SOURCE_DIR=\"/downloads\" \\\n  -e DESTINATION_DIR=\"/library\" \\\n  -e CACHE_DIR=\"/cache\" \\\n  -v /path/to/your/config:/config \\\n  -v /path/to/downloads:/downloads \\\n  -v /path/to/library:/library \\\n  -v /path/to/cache:/cache \\\n  ghcr.io/s0len/playbook:latest\n</code></pre> <p>Replace <code>/path/to/...</code> with your actual host directories. This command:</p> <ul> <li>Runs once and exits (<code>--rm</code>)</li> <li>Shows what would happen without moving files (<code>DRY_RUN=true</code>)</li> <li>Prints detailed matching logic (<code>VERBOSE=true</code>)</li> <li>Validates config and metadata access</li> </ul>"},{"location":"getting-started/#step-2-run-continuously","title":"Step 2: Run continuously","text":"<p>Once the dry-run looks good, run detached:</p> <pre><code>docker run -d \\\n  --name playbook \\\n  -e TZ=\"UTC\" \\\n  -e SOURCE_DIR=\"/downloads\" \\\n  -e DESTINATION_DIR=\"/library\" \\\n  -e CACHE_DIR=\"/cache\" \\\n  -e WATCH_MODE=true \\\n  -v /path/to/your/config:/config \\\n  -v /path/to/downloads:/downloads \\\n  -v /path/to/library:/library \\\n  -v /path/to/cache:/cache \\\n  ghcr.io/s0len/playbook:latest\n</code></pre>"},{"location":"getting-started/#environment-variables-reference","title":"Environment Variables Reference","text":"Variable Purpose Example <code>SOURCE_DIR</code> Where downloads land (required) <code>/downloads</code> <code>DESTINATION_DIR</code> Plex library path (required) <code>/library</code> <code>CACHE_DIR</code> Metadata/state storage (required) <code>/cache</code> <code>WATCH_MODE</code> Keep running and watch for new files <code>true</code> <code>DRY_RUN</code> Simulate without moving files <code>true</code> <code>VERBOSE</code> Detailed matching logs <code>true</code> <code>LOG_LEVEL</code> Logging verbosity <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>"},{"location":"getting-started/#monitoring","title":"Monitoring","text":"<pre><code># Tail live logs\ndocker logs -f playbook\n\n# Check container status\ndocker ps -a | grep playbook\n</code></pre>"},{"location":"getting-started/#option-b-python-environment","title":"Option B: Python Environment","text":"<p>Use this method for local development, testing, or if you prefer managing Python dependencies directly:</p>"},{"location":"getting-started/#step-1-create-a-virtual-environment","title":"Step 1: Create a virtual environment","text":"<pre><code>python3 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#step-2-validate-your-config","title":"Step 2: Validate your config","text":"<pre><code>python -m playbook.cli validate-config --config /path/to/playbook.yaml --diff-sample\n</code></pre>"},{"location":"getting-started/#step-3-test-with-a-dry-run","title":"Step 3: Test with a dry-run","text":"<pre><code>export SOURCE_DIR=\"/path/to/downloads\"\nexport DESTINATION_DIR=\"/path/to/library\"\nexport CACHE_DIR=\"/path/to/cache\"\n\npython -m playbook.cli process --config /path/to/playbook.yaml --dry-run --verbose\n</code></pre>"},{"location":"getting-started/#step-4-run-continuously-watcher-mode","title":"Step 4: Run continuously (watcher mode)","text":"<pre><code>export WATCH_MODE=true\npython -m playbook.cli watch --config /path/to/playbook.yaml\n</code></pre>"},{"location":"getting-started/#useful-environment-variables","title":"Useful Environment Variables","text":"<p>Same as Docker \u2013 set <code>SOURCE_DIR</code>, <code>DESTINATION_DIR</code>, <code>CACHE_DIR</code>, <code>LOG_LEVEL</code>, <code>VERBOSE</code>, etc. before running commands.</p>"},{"location":"getting-started/#option-c-kubernetes-flux-helmrelease","title":"Option C: Kubernetes (Flux HelmRelease)","text":"<p>Use this method if you're running a Flux-based Kubernetes cluster. This example uses the bjw-s/app-template chart to deploy Playbook as a single-pod deployment with persistent storage:</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster with Flux installed</li> <li>Persistent storage provisioner (or NFS access)</li> <li>bjw-s app-template OCIRepository configured in Flux</li> </ul> <pre><code>---\n# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: &amp;app playbook\nspec:\n  interval: 30m\n  chartRef:\n    kind: OCIRepository\n    name: app-template\n  install:\n    remediation:\n      retries: 3\n  upgrade:\n    cleanupOnFail: true\n    remediation:\n      strategy: rollback\n      retries: 3\n\n  values:\n    serviceAccount:\n      playbook: {}\n\n    controllers:\n      main:\n        serviceAccount:\n          identifier: playbook\n        type: deployment\n        containers:\n          app:\n            image:\n              repository: ghcr.io/s0len/playbook\n              tag: develop@sha256:6248cac4f5aeb9403a88f919e522e96291e3c93eb018c289aff4dbfef92ec5fa\n              pullPolicy: Always\n            env:\n              CACHE_DIR: /settings/cache\n              CLEAR_PROCESSED_CACHE: false\n              CONFIG_PATH: /config/config.yaml\n              DRY_RUN: false\n              DESTINATION_DIR: /data/media/sport\n              LOG_DIR: /tmp\n              LOG_LEVEL: INFO\n              SOURCE_DIR: /data/torrents/sport\n            envFrom:\n              - secretRef:\n                  name: playbook-secret\n            securityContext:\n              privileged: false\n        annotations:\n          reloader.stakater.com/auto: \"true\"\n\n    defaultPodOptions:\n      automountServiceAccountToken: true\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 568\n        runAsGroup: 568\n        runAsNonRoot: true\n        fsGroup: 568\n\n    persistence:\n      settings:\n        existingClaim: playbook-settings\n        globalMounts:\n          - path: /settings\n            readOnly: false\n\n      tmp:\n        type: emptyDir\n        medium: Memory\n        globalMounts:\n          - path: /tmp\n\n      data:\n        type: nfs\n        server: \"${TRUENAS_IP}\"\n        path: /mnt/rust/data\n        globalMounts:\n          - path: /data\n            readOnly: false\n\n      config:\n        type: configMap\n        name: playbook-configmap\n        globalMounts:\n          - path: /config/config.yaml\n            subPath: config.yaml\n            readOnly: true\n</code></pre>"},{"location":"getting-started/#key-configuration-notes","title":"Key Configuration Notes","text":"<ul> <li>Image tag: Use <code>latest</code> for stable releases or <code>develop</code> for bleeding edge (pin to digest for reproducibility)</li> <li>Environment variables: Set <code>SOURCE_DIR</code>, <code>DESTINATION_DIR</code>, <code>CACHE_DIR</code> to match your persistent volume mount paths</li> <li>Config file: Mount via ConfigMap or Secret (this example uses a ConfigMap at <code>/config/config.yaml</code>)</li> <li>Secrets: Store sensitive values (API keys, webhook URLs) in a <code>playbook-secret</code>, reference it with <code>envFrom</code>, and point <code>notifications.targets[].webhook_env</code> at the relevant variable (e.g., <code>DISCORD_WEBHOOK_URL</code>)</li> <li>Persistent storage: Cache directory should survive pod restarts (use PVC or NFS)</li> <li>Watcher mode: Enable <code>WATCH_MODE=true</code> or <code>file_watcher.enabled: true</code> in config for continuous processing</li> <li>Automatic reloads: Add <code>reloader.stakater.com/auto: \"true\"</code> annotation to restart when ConfigMap changes</li> </ul>"},{"location":"getting-started/#deployment-checklist","title":"Deployment Checklist","text":"<ol> <li>Create the secret: <code>kubectl create secret generic playbook-secret --from-literal=DISCORD_WEBHOOK=https://...</code></li> <li>Create the ConfigMap with your <code>playbook.yaml</code> content: <code>kubectl create configmap playbook-configmap --from-file=config.yaml=playbook.yaml</code></li> <li>Apply the HelmRelease manifest</li> <li>Check logs: <code>kubectl logs -f deployment/playbook</code></li> <li>Verify processing: watch destination directory for new files</li> </ol>"},{"location":"getting-started/#first-run-checklist","title":"First Run Checklist","text":"<p>Follow these steps in order to ensure everything works before running automatically:</p>"},{"location":"getting-started/#1-validate-your-config","title":"1. Validate Your Config","text":"<pre><code># Docker\ndocker run --rm -it \\\n  -v /path/to/your/config:/config \\\n  ghcr.io/s0len/playbook:latest validate-config --config /config/playbook.yaml --diff-sample\n\n# Python\npython -m playbook.cli validate-config --config playbook.yaml --diff-sample\n</code></pre> <p>This command:</p> <ul> <li>Checks YAML syntax and required fields</li> <li>Validates pattern templates and matching logic</li> <li>Shows differences from the sample config</li> <li>Confirms metadata URLs are accessible</li> </ul>"},{"location":"getting-started/#2-run-a-dry-run","title":"2. Run a Dry-Run","text":"<pre><code># Docker\ndocker run --rm -it \\\n  -e DRY_RUN=true \\\n  -e VERBOSE=true \\\n  -e SOURCE_DIR=\"/downloads\" \\\n  -e DESTINATION_DIR=\"/library\" \\\n  -e CACHE_DIR=\"/cache\" \\\n  -v /path/to/your/config:/config \\\n  -v /path/to/downloads:/downloads \\\n  -v /path/to/library:/library \\\n  -v /path/to/cache:/cache \\\n  ghcr.io/s0len/playbook:latest\n\n# Python\nexport SOURCE_DIR=\"/path/to/downloads\"\nexport DESTINATION_DIR=\"/path/to/library\"\nexport CACHE_DIR=\"/path/to/cache\"\npython -m playbook.cli process --config playbook.yaml --dry-run --verbose\n</code></pre> <p>Look for:</p> <ul> <li>Matched files: Confirm the pattern matched expected downloads</li> <li>Destination paths: Verify the Plex folder structure looks correct</li> <li>Skipped files: Understand why some files weren't processed</li> <li>API responses: Check that TVSportsDB metadata loaded successfully</li> </ul>"},{"location":"getting-started/#3-review-logs","title":"3. Review Logs","text":"<p>Check <code>playbook.log</code> (or <code>docker logs</code>) for:</p> <ul> <li>Files successfully matched and their destination paths</li> <li>Any pattern matching failures or unmatched files</li> <li>Kometa/Autoscan trigger notifications (if enabled)</li> <li>Cache warming progress (first run downloads all metadata)</li> </ul>"},{"location":"getting-started/#4-test-the-real-thing","title":"4. Test the Real Thing","text":"<p>Remove <code>DRY_RUN=true</code> and run a single batch:</p> <pre><code># Docker (one-shot)\ndocker run --rm -it \\\n  -e SOURCE_DIR=\"/downloads\" \\\n  -e DESTINATION_DIR=\"/library\" \\\n  -e CACHE_DIR=\"/cache\" \\\n  -v /path/to/your/config:/config \\\n  -v /path/to/downloads:/downloads \\\n  -v /path/to/library:/library \\\n  -v /path/to/cache:/cache \\\n  ghcr.io/s0len/playbook:latest\n</code></pre> <p>Verify:</p> <ul> <li>Files moved to correct destination folders</li> <li>Filenames follow Plex conventions (e.g., <code>S2024E05 - Round 5 - Monaco Grand Prix.mkv</code>)</li> <li>Original source files cleaned up (or preserved if <code>preserve_source: true</code>)</li> </ul>"},{"location":"getting-started/#5-point-plex-at-the-destination","title":"5. Point Plex at the Destination","text":"<p>Add your destination directory as a TV library in Plex:</p> <ul> <li>Library type: TV Shows</li> <li>Scanner: Plex Series Scanner</li> <li>Agent: Plex TV Series (metadata will be enriched by Kometa)</li> </ul> <p>Scan the library and confirm shows/seasons/episodes appear.</p>"},{"location":"getting-started/#6-enable-continuous-processing","title":"6. Enable Continuous Processing","text":"<p>Once everything looks good, enable watcher mode or schedule regular runs:</p>"},{"location":"getting-started/#docker-detached-with-watcher","title":"Docker (detached with watcher)","text":"<pre><code>docker run -d \\\n  --name playbook \\\n  -e WATCH_MODE=true \\\n  -e SOURCE_DIR=\"/downloads\" \\\n  -e DESTINATION_DIR=\"/library\" \\\n  -e CACHE_DIR=\"/cache\" \\\n  -v /path/to/your/config:/config \\\n  -v /path/to/downloads:/downloads \\\n  -v /path/to/library:/library \\\n  -v /path/to/cache:/cache \\\n  ghcr.io/s0len/playbook:latest\n</code></pre>"},{"location":"getting-started/#python-watcher-mode","title":"Python (watcher mode)","text":"<pre><code>export WATCH_MODE=true\npython -m playbook.cli watch --config playbook.yaml\n</code></pre>"},{"location":"getting-started/#kubernetes","title":"Kubernetes","text":"<p>Already running continuously with the HelmRelease</p>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration Guide \u2013 deep dive into all YAML options, pattern syntax, and templating</li> <li>Integrations \u2013 connect Kometa for metadata, Autoscan for instant Plex updates, Autobrr for automatic downloads</li> <li>Recipes \u2013 sport-specific examples (F1, MotoGP, UFC, etc.) and custom pattern tutorials</li> <li>Troubleshooting \u2013 common issues, cache resets, debugging tips</li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<p>Playbook focuses on filesystem organization and leaves downloading, metadata enrichment, and library refreshes to the tools that already excel at those jobs. This guide summarizes the supported integrations.</p>"},{"location":"integrations/#integration-overview","title":"Integration Overview","text":"Integration Purpose Quick link Plex Metadata Sync Pushes show/season/episode metadata from TVSportsDB directly to Plex. Plex Metadata Sync Kometa Applies metadata feeds so Plex gets canonical titles/artwork. Kometa Metadata Kometa trigger Automatically re-runs Kometa whenever Playbook links new files. Kometa Triggering Autobrr Automates torrent intake so Playbook always has fresh downloads to normalize. Autobrr Automation Plex + Autoscan Keeps your TV library and metadata agents refreshed the moment Playbook writes new files. Plex Library Setup / Autoscan Hooks"},{"location":"integrations/#plex-metadata-sync","title":"Plex Metadata Sync","text":"<p>Playbook can push show, season, and episode metadata directly from TVSportsDB to Plex. This includes:</p> <ul> <li>Titles and sort titles - Preserves original casing (e.g., \"NTT IndyCar Series\" instead of Plex's normalized \"Ntt Indycar Series\")</li> <li>Summaries - Episode and season descriptions</li> <li>Dates - Originally available dates for proper episode ordering</li> <li>Artwork - Posters and backgrounds from TVSportsDB</li> </ul>"},{"location":"integrations/#configuration","title":"Configuration","text":"<p>Enable Plex metadata sync in your config:</p> <pre><code>settings:\n  plex_metadata_sync:\n    enabled: true\n    url: ${PLEX_URL:-http://plex:32400}\n    token: ${PLEX_TOKEN:-}\n    library_id: ${PLEX_LIBRARY_ID:-}      # Optional: faster lookup by ID\n    library_name: ${PLEX_LIBRARY_NAME:-TV Shows}\n    timeout: 15\n    force: false       # Force all updates even if unchanged\n    dry_run: false     # Log only, no Plex writes\n    sports: []         # Limit to specific sport IDs; empty = all sports\n    scan_wait: 5       # Seconds to wait after triggering library scan\n</code></pre>"},{"location":"integrations/#how-it-works","title":"How It Works","text":"<ol> <li>Automatic sync - When Playbook processes new files, it automatically syncs metadata for affected sports</li> <li>Fingerprint-based detection - Only syncs when metadata actually changes (based on content fingerprints)</li> <li>First-time sync - Sports that have never been synced will auto-sync on first run</li> <li>Field locking - Locks Plex fields after updating to prevent metadata refresh from overwriting</li> </ol>"},{"location":"integrations/#environment-variables","title":"Environment Variables","text":"Variable Purpose <code>PLEX_URL</code> Plex server URL <code>PLEX_TOKEN</code> Plex authentication token <code>PLEX_LIBRARY_ID</code> Target library ID <code>PLEX_LIBRARY_NAME</code> Target library name (fallback) <code>PLEX_SYNC_ENABLED</code> Enable/disable sync <code>PLEX_FORCE</code> Force all updates <code>PLEX_SYNC_DRY_RUN</code> Dry-run mode <code>PLEX_SPORTS</code> Comma-separated sport IDs to sync"},{"location":"integrations/#relationship-with-kometa","title":"Relationship with Kometa","text":"<p>Plex Metadata Sync and Kometa serve complementary purposes:</p> <ul> <li>Plex Metadata Sync - Pushes TVSportsDB metadata (titles, summaries, artwork) to Plex</li> <li>Kometa - Creates collections, applies overlays, and handles advanced library management</li> </ul> <p>You can use both together: Plex Metadata Sync keeps episode data fresh, while Kometa handles collections and visual customizations.</p>"},{"location":"integrations/#kometa-metadata","title":"Kometa Metadata","text":"<p>Point Kometa at metadata YAML feeds so Plex shows canonical titles, posters, and collections. Start by wiring a <code>libraries</code> block:</p> <pre><code>libraries:\n  Sport:\n    metadata_files:\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/formula1/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/formula-e/2025-2026.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/indycar-series/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/isle-of-man-tt.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/moto2/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/moto3/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/motogp/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/nba/2025-2026.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/nfl/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/premier-league/2025-2026.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/uefa-champions-league/2025-2026.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/ufc/2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/wsbk-2025.yaml\n      - url: https://raw.githubusercontent.com/s0len/meta-manager-config/refs/heads/main/metadata/wssp-2025.yaml\n</code></pre> <ol> <li>List the metadata YAML feeds for your sports (feeds live under <code>s0len/meta-manager-config</code>).</li> <li>Use a single Kometa library (e.g., <code>Sport</code>) to collect all sports series; Kometa handles per-show organization via the YAML metadata.</li> <li>Schedule Kometa normally (cron, k8s CronJob) so it still refreshes on its own, then layer Playbook triggers for instant updates.</li> </ol> <p>Note: Playbook now fetches metadata from TVSportsDB API, not YAML files. Kometa can still use YAML feeds for its own metadata operations\u2014they're maintained in parallel.</p>"},{"location":"integrations/#kometa-triggering","title":"Kometa Triggering from Playbook","text":"<p>Configure <code>settings.kometa_trigger</code> so Playbook nudges Kometa only when new files were linked. Trigger modes:</p> Mode When to use Requirements <code>kubernetes</code> Playbook and Kometa live in the same cluster. Clone an existing CronJob spec. Service account with permission to read/clone the CronJob. <code>docker</code> Kometa runs as a standalone container or via Docker Compose. Mount Docker socket &amp; binaries into the Playbook container, or run Playbook on the host. <code>docker exec</code> Kometa is already running (<code>docker-compose up</code>). Set <code>docker.container_name</code> (plus optional <code>exec_python</code>/<code>exec_script</code>)."},{"location":"integrations/#kubernetes-cronjob-trigger","title":"Kubernetes CronJob Trigger","text":"<pre><code>settings:\n  kometa_trigger:\n    enabled: true\n    mode: kubernetes\n    namespace: media\n    cronjob_name: kometa-sport\n    job_name_prefix: kometa-sport-triggered-by-playbook\n</code></pre> <p>Playbook clones the CronJob's <code>jobTemplate</code>, labels jobs with <code>trigger=playbook</code>, and logs the job name so you can <code>kubectl logs job/&lt;name&gt;</code> later.</p>"},{"location":"integrations/#docker-run-exec-trigger","title":"Docker Run / Exec Trigger","text":"<pre><code>settings:\n  kometa_trigger:\n    enabled: true\n    mode: docker\n    docker:\n      binary: docker\n      image: kometateam/kometa\n      config_path: /srv/media/Kometa/config\n      libraries: \"Sports|TV Shows - 4K\"\n      extra_args:\n        - --config\n        - /config/config.yml\n</code></pre> <p>Under the hood Playbook runs:</p> <pre><code>docker run --rm \\\n  -v \"/srv/media/Kometa/config:/config:rw\" \\\n  kometateam/kometa \\\n  --run-libraries \"Sports|TV Shows - 4K\" \\\n  --config /config/config.yml\n</code></pre> <p>Already running Kometa via Docker Compose? Set <code>docker.container_name</code> (plus optional <code>exec_python</code> / <code>exec_script</code>) and Playbook will call <code>docker exec</code> instead of spinning up a new container. For absolute control, provide <code>docker.exec_command</code> (e.g., <code>[\"python3\", \"/app/kometa/kometa.py\"]</code>) and Playbook will append <code>libraries</code>/<code>extra_args</code>.</p>"},{"location":"integrations/#docker-prerequisites-inside-the-playbook-container","title":"Docker prerequisites inside the Playbook container","text":"<ul> <li>Mount <code>/var/run/docker.sock</code> so the daemon is reachable.</li> <li>Mount the client binaries (paths differ per host; use <code>command -v docker</code>). Example:</li> </ul> <pre><code>-v $(command -v docker):/usr/local/bin/docker \\\n-v $(command -v com.docker.cli):/usr/local/bin/com.docker.cli\n</code></pre>"},{"location":"integrations/#manual-trigger-cli","title":"Manual Trigger CLI","text":"<p>Use <code>python -m playbook.cli kometa-trigger --config /config/playbook.yaml --mode docker</code> when you want to test the integration without a full ingest run. Logs are captured inside <code>playbook.log</code>, so failed triggers show up alongside the main processor output. Combine it with <code>--verbose --log-level DEBUG</code> to watch the exact docker command invoked.</p>"},{"location":"integrations/#kometa-troubleshooting","title":"Kometa troubleshooting","text":"<ul> <li><code>Kometa trigger is disabled</code> \u2013 enable <code>settings.kometa_trigger.enabled</code> or pass <code>--mode docker</code> / <code>--mode kubernetes</code> to the CLI.</li> <li><code>Kometa docker trigger requires access to the Docker socket</code> \u2013 mount <code>/var/run/docker.sock</code> plus the Docker client binary.</li> <li>Stuck jobs in Kubernetes? <code>kubectl delete job -l trigger=playbook</code> to clear out previous runs before trying again.</li> <li>Need a dry run? Set <code>kometa_trigger.enabled: false</code>, run Playbook once to ensure filenames look good, then re-enable and re-run.</li> </ul>"},{"location":"integrations/#autobrr-download-automation","title":"Autobrr Download Automation","text":"<p>Playbook expects files to appear in <code>SOURCE_DIR</code>. When you want to automate torrent grabs, Autobrr pairs well with the built-in pattern packs.</p>"},{"location":"integrations/#basic-setup","title":"Basic Setup","text":"<ol> <li>Create a filter per sport (e.g., <code>F1 1080p MWR</code>, <code>EPL 1080p NiGHTNiNJAS</code>).</li> <li>Select trackers that carry those releases.</li> <li>Under Advanced \u2192 Release names \u2192 Match releases, paste regexes that encode sport/year/resolution/release-group.</li> <li>Assign a dedicated category/tag so your downloader routes sports torrents into the Playbook <code>SOURCE_DIR</code>.</li> <li>Use Autobrr\u2019s <code>Actions</code> to push directly into qBittorrent/Deluge, or fire off a webhook/exec script for bespoke flows.</li> </ol> <p>Sample filter (partial TOML):</p> <p><code>--8&lt;-- \"snippets/autobrr-filter.md\"</code></p>"},{"location":"integrations/#sample-regexes","title":"Sample Regexes","text":"<pre><code># Premier League (EPL) 1080p releases by NiGHTNiNJAS\nepl.*1080p.*nightninjas\n\nPlaybook also swallows the non-dotted drops (`EPL 2025 Fulham vs Manchester City 02 12 \u2026`) and normalizes common nicknames (`Leeds`, `Man City`, etc.) so you can keep a single filter for every encoder variant.\n\n# Formula 1 multi-session weekends by MWR\n(F1|Formula.*1).*\\d{4}.Round\\d+.*[^.]+\\.*?(Drivers.*Press.*Conference|Weekend.*Warm.*Up|FP\\d?|Practice|Sprint.Qualifying|Sprint|Qualifying|Pre.Qualifying|Post.Qualifying|Race|Pre.Race|Post.Race|Sprint.Race|Feature.*Race).*1080p.*MWR\n\n# Formula E by MWR\nformulae\\.\\d{4}\\.round\\d+\\.(?:[A-Za-z]+(?:\\.[A-Za-z]+)?)\\.(?:preview.show|qualifying|race)\\..*h264.*-mwr\n\n# IndyCar by MWR\nindycar.*\\d{4}\\.round\\d+\\.(?:[A-Za-z]+(?:\\.[A-Za-z]+)?)\\.(?:qualifying|race)\\..*h264.*-MWR\n\n# Isle of Man TT by DNU\nisle.of.man.tt.*DNU\n\n# MotoGP by DNU\nmotogp.*\\d{4}.*round\\d.*((fp\\d?|practice|sprint|qualifying|q1|q2|race)).*DNU\n\n# NBA 1080p by GAMETiME\nnba.*1080p.*gametime\n\n# NHL RS 60fps feeds\nnhl.*rs.*(720p|1080p).*en60fps\n\n# NFL by NiGHTNiNJAS\nnfl.*nightninjas\n\n# UFC by VERUM\nufc[ ._-].*?\\d{3}.*verum\n\n# WorldSBK / WorldSSP / WorldSSP300 by MWR\n(wsbk|wssp|wssp300).*\\d{4}.round\\d+.[^.]+.(fp\\d?|season.preview|superpole|race.one|race.two|war.up(one|two)?|weekend.highlights).*h264.*mwr\n</code></pre> <p>Tips:</p> <ul> <li>UFC releases now must include the matchup slug (e.g., <code>UFC 322 Della Maddalena vs Makhachev</code>) so Playbook can align each file with the correct metadata season.</li> <li>Test regexes via Autobrr\u2019s built-in tester or an interactive <code>ripgrep --pcre2</code> session before enabling filters.</li> <li>When multiple release groups upload the same events, use Autobrr\u2019s <code>required_words</code>/<code>excluded_words</code> to prefer trusted encoders.</li> <li>Keep filters narrow\u2014use separate rules for 1080p vs 2160p, English vs multi-audio, etc.\u2014so Playbook can rely on deterministic naming.</li> </ul>"},{"location":"integrations/#plex-library-setup","title":"Plex Library Setup","text":"<ol> <li>In the Plex UI, Libraries \u2192 Add Library \u2192 TV Shows.</li> <li>Name it something like <code>Sports</code>.</li> <li>Point the folder at the same <code>DESTINATION_DIR</code> (or the sports subdirectory) that Playbook writes to.</li> <li>Under Advanced:</li> <li>Scanner: <code>Plex Series Scanner</code></li> <li>Agent: <code>Personal Media Shows</code></li> <li>Episode sorting: <code>Newest first</code></li> <li>Run Scan Library Files after Playbook populates the destination folder.</li> </ol> <p>This pairing ensures Plex treats every season/session as canonical TV episodes while Kometa layers metadata on top.</p> <p>When using Kometa:</p> <ul> <li>Create the library first, run a manual Scan Library Files, then let Kometa populate posters/collections.</li> <li>Keep the Playbook destination mounted read-only inside Plex if you want extra protection against accidental edits.</li> <li>Pair Plex with Autoscan to avoid delayed library scans.</li> </ul>"},{"location":"integrations/#autoscan-hooks","title":"Autoscan Hooks","text":"<p>Add an <code>autoscan</code> notification target to retrigger Plex/Emby/Jellyfin scans immediately after new files are linked:</p> <p><code>--8&lt;-- \"snippets/notifications-autoscan.md\"</code></p>"},{"location":"integrations/#security-warning-ssltls-verification","title":"\u26a0\ufe0f Security Warning: SSL/TLS Verification","text":"<p>IMPORTANT: The <code>verify_ssl</code> setting controls SSL/TLS certificate verification for HTTPS connections to Autoscan. Disabling this verification (<code>verify_ssl: false</code>) exposes your system to man-in-the-middle (MITM) attacks where an attacker can intercept and modify the communication between Playbook and Autoscan.</p> <ul> <li>Production environments: ALWAYS keep <code>verify_ssl: true</code> (the default)</li> <li>Development/testing only: <code>verify_ssl: false</code> may be used temporarily with self-signed certificates, but you should properly configure certificate trust stores instead</li> <li>Better alternatives: Add self-signed certificates to your system's trust store or use a proper CA-signed certificate rather than disabling verification</li> </ul> <p>Guidelines:</p> <ul> <li><code>rewrite</code> entries translate Playbook's container paths into whatever Autoscan/Plex can see (add as many mappings as needed).</li> <li>Combine Autoscan pings with watcher mode for near-instant Plex updates\u2014new files drop, Playbook links them, Autoscan triggers a scan.</li> <li>Want extra resiliency? Keep one Autoscan target per Plex server/library pair so failures are isolated.</li> </ul> <p>Need end-to-end walkthroughs? Jump to Recipes &amp; How-tos.</p>"},{"location":"operations/","title":"Operations &amp; Run Modes","text":"<p>The CLI powers every deployment path\u2014Docker entrypoint, Kubernetes container, or ad-hoc batch run. This guide covers run modes, flags, logging, and day-2 operations.</p>"},{"location":"operations/#run-modes","title":"Run Modes","text":"Mode How to enable Best for Batch Default CLI behavior (<code>python -m playbook.cli</code>) One-off reorg runs and cron jobs Watcher <code>--watch</code> flag or <code>WATCH_MODE=true</code> (also controlled via <code>settings.file_watcher.enabled</code>) Always-on ingestion, reacts to filesystem events Validate <code>python -m playbook.cli validate-config --config ...</code> CI gates + local smoke tests <p>Batch mode exits after a single pass. Watcher mode keeps the process alive, listening for <code>create</code>, <code>modify</code>, and <code>move</code> events underneath <code>source_dir</code> (or <code>file_watcher.paths</code>). Use <code>file_watcher.debounce_seconds</code> to batch bursts of events, and <code>file_watcher.reconcile_interval</code> to force periodic full scans in case the platform drops events.</p>"},{"location":"operations/#cli-flags-environment-variables","title":"CLI Flags &amp; Environment Variables","text":"CLI Flag Environment Default Notes <code>--config PATH</code> <code>CONFIG_PATH</code> <code>/config/playbook.yaml</code> Path to the YAML config. <code>--dry-run</code> <code>DRY_RUN</code> Inherits <code>settings.dry_run</code> Force no-write mode. <code>--verbose</code> <code>VERBOSE</code> / <code>DEBUG</code> <code>false</code> Enables console <code>DEBUG</code> output. <code>--log-level LEVEL</code> <code>LOG_LEVEL</code> <code>INFO</code> (or <code>DEBUG</code> with <code>--verbose</code>) File log level. <code>--console-level LEVEL</code> <code>CONSOLE_LEVEL</code> Matches file level Console log level. <code>--log-file PATH</code> <code>LOG_FILE</code> / <code>LOG_DIR</code> <code>./playbook.log</code> Rotates to <code>*.previous</code> on start. <code>--trace-matches</code> / <code>--explain</code> \u2014 <code>false</code> Capture per-file trace JSON under <code>cache_dir/traces</code>. <code>--trace-output PATH</code> \u2014 <code>cache_dir/traces</code> Custom directory for trace JSONs (implies <code>--trace-matches</code>). <code>--clear-processed-cache</code> <code>CLEAR_PROCESSED_CACHE</code> <code>false</code> Resets processed file cache before processing. <code>--watch</code> <code>WATCH_MODE=true</code> <code>settings.file_watcher.enabled</code> Force watcher mode on. <code>--no-watch</code> <code>WATCH_MODE=false</code> <code>false</code> Disable watcher mode even if config enables it. <p>Environment variables override config defaults; CLI flags override both. <code>SOURCE_DIR</code>, <code>DESTINATION_DIR</code>, and <code>CACHE_DIR</code> also override the <code>settings</code> block at runtime, which is handy for per-environment deployments.</p>"},{"location":"operations/#mode-playbooks","title":"Mode playbooks","text":""},{"location":"operations/#batch-runs-via-cronsystemd-timer","title":"Batch runs via cron/systemd timer","text":"<p>Use cron (or a systemd timer) to kick off a fresh container/CLI run on a schedule. The example below dry-runs every hour and exits cleanly when finished:</p> <p><code>--8&lt;-- \"snippets/cron-batch-job.md\"</code></p> <p>Keep <code>LOG_DIR</code> and <code>CACHE_DIR</code> on persistent volumes so subsequent runs stay warm.</p>"},{"location":"operations/#always-on-watcher-via-systemd","title":"Always-on watcher via systemd","text":"<p>When running natively on a host, wrap the CLI in a systemd service so it restarts after crashes and logs to <code>journalctl</code> while still writing <code>playbook.log</code>:</p> <p><code>--8&lt;-- \"snippets/systemd-watcher-service.md\"</code></p> <p>Store secrets in <code>/etc/playbook.env</code> (referenced by <code>EnvironmentFile</code>) and keep the virtualenv pinned to a known path.</p>"},{"location":"operations/#kubernetes-patterns","title":"Kubernetes patterns","text":"<ul> <li>CronJob \u2013 mirrors the batch run model; the container runs once and exits. Ideal for nightly/weekly cleanups.</li> <li>Deployment / Flux HelmRelease \u2013 keeps the watcher alive. Use <code>WATCH_MODE=true</code>, persistent cache/log PVCs, and <code>reloader.stakater.com/auto: \"true\"</code> so config changes hot-reload.</li> </ul> <p>Examples for both live in Getting Started.</p>"},{"location":"operations/#subcommands","title":"Subcommands","text":"<ul> <li><code>python -m playbook.cli validate-config --config \u2026 --diff-sample --show-trace</code>   CI-friendly validation that enforces schema checks and surfaces diffs against <code>config/playbook.sample.yaml</code>.</li> <li><code>python -m playbook.cli kometa-trigger --config \u2026 --mode docker</code>   Triggers Kometa once without running the processor. Useful when debugging trigger failures or forcing a metadata refresh after a manual ingest.</li> </ul>"},{"location":"operations/#plex-metadata-sync","title":"Plex Metadata Sync","text":"<p>Playbook can push show/season/episode metadata (titles, sort titles, summaries, dates, posters, backgrounds) directly to Plex after processing files. This eliminates the need for Kometa in many cases.</p> <p>Configuration (<code>settings.plex_metadata_sync</code>):</p> Setting Env Override Description <code>enabled</code> <code>PLEX_SYNC_ENABLED</code> Enable Plex sync (default: false) <code>url</code> <code>PLEX_URL</code> Plex server URL (e.g., <code>http://plex:32400</code>) <code>token</code> <code>PLEX_TOKEN</code> Plex auth token <code>library_id</code> <code>PLEX_LIBRARY_ID</code> Target library section ID <code>library_name</code> <code>PLEX_LIBRARY_NAME</code> Target library name (used if ID not set) <code>timeout</code> <code>PLEX_TIMEOUT</code> HTTP timeout in seconds (default: 15) <code>force</code> <code>PLEX_FORCE</code> Force updates even when metadata unchanged <code>dry_run</code> <code>PLEX_SYNC_DRY_RUN</code> Log updates without making API calls <code>sports</code> <code>PLEX_SPORTS</code> Comma-separated list of sport IDs to sync <code>scan_wait</code> <code>PLEX_SCAN_WAIT</code> Seconds to wait after library scan (default: 5, set 0 to skip) <p>How it works:</p> <ol> <li>Automatic: When enabled, Plex sync runs automatically after file processing.</li> <li>Smart sync decision: Sync runs when:</li> <li>New files were processed</li> <li>Metadata changed in remote YAML</li> <li>Sports have never been synced to Plex (first-time sync)</li> <li>Force mode is enabled</li> <li>Change detection: Uses fingerprint-based detection\u2014shows/seasons only update when metadata changes; episodes update when their content changes.</li> <li>First-time sync: If a sport has never been synced to Plex, it will sync automatically even if no new files were processed. This ensures existing content gets proper metadata.</li> <li>Field locking: Sets <code>{field}.locked=1</code> to prevent Plex agents from overwriting your custom metadata.</li> <li>Rate limiting: Built-in rate limiting and retry logic for resilient API calls.</li> <li>Security: Token passed via header (not URL query params); URLs sanitized in logs.</li> </ol> <p>Manual execution:</p> <pre><code>python -m playbook.plex_metadata_sync --config /config/playbook.yaml --verbose\n</code></pre> <p>State files (stored in <code>cache_dir/state/</code>): - <code>plex-metadata-hashes.json</code>: Fingerprint cache for change detection - <code>plex-sync-state.json</code>: Tracks which sports have been synced to Plex</p>"},{"location":"operations/#logging-observability","title":"Logging &amp; Observability","text":"<ul> <li>Logs use a multi-line block layout (timestamp, header, aligned key/value pairs) for rapid scanning.</li> <li>INFO-level runs summarize totals per sport/source; add <code>--verbose</code>/<code>LOG_LEVEL=DEBUG</code> for per-file diagnostics.</li> <li>Each pass ends with a <code>Run Recap</code> block (duration, totals, Plex sync status, destination samples).</li> <li>On every start, the previous log rotates to <code>playbook.log.previous</code>. Persist <code>/var/log/playbook</code> (Docker) or whatever <code>LOG_DIR</code> you choose.</li> <li>Set <code>LOG_LEVEL=WARNING</code> (or higher) to cut down on noise for steady-state watcher deployments.</li> <li><code>PLAIN_CONSOLE_LOGS=true</code> forces plain text output (handy for syslog collectors); <code>RICH_CONSOLE_LOGS=true</code> forces Rich formatting even when the console isn\u2019t a TTY.</li> <li>Use separate <code>LOG_DIR</code> mounts per environment and ship the log files to your preferred log stack (Vector, Fluent Bit, Loki, etc.).</li> </ul>"},{"location":"operations/#tracing-diagnostics","title":"Tracing &amp; diagnostics","text":"<ul> <li><code>--trace-matches</code> (or <code>--explain</code>) writes JSON artifacts per processed file so you can audit regex captures, selectors, and template output.</li> <li><code>--trace-output /path/to/dir</code> stores those JSONs somewhere other than <code>cache_dir/traces</code>.</li> <li><code>--clear-processed-cache</code> forces Playbook to treat every file as new; pair it with <code>--dry-run</code> when validating a new config so you see complete notifications and Kometa trigger previews without touching the filesystem.</li> <li>Combine <code>--dry-run --verbose --trace-matches</code> to capture a full story: console logs, persistent logs, and JSON traces for each match.</li> <li>For watcher deployments, schedule periodic <code>validate-config</code> runs in CI so schema regressions surface before you roll containers.</li> </ul>"},{"location":"operations/#monitoring-hooks","title":"Monitoring Hooks","text":"<ul> <li><code>settings.kometa_trigger</code> nudges Kometa after each ingest cycle. Modes: <code>kubernetes</code> (clone a CronJob) or <code>docker</code> (run/exec a container). Detailed examples live in Integrations.</li> <li><code>notifications.targets</code> can ping Autoscan immediately after new files appear so Plex rescans folders without manual input.</li> <li>Add <code>notifications.targets</code> entries for Discord/Slack/webhooks to receive summaries per run or per day.</li> </ul>"},{"location":"operations/#directory-conventions","title":"Directory Conventions","text":"<p>Hardlinks are the default action, so you retain the original downloads while presenting a clean Plex library. A typical layout after a Formula 1 weekend:</p> <pre><code>Formula 1 2025/\n\u2514\u2500\u2500 01 Bahrain Grand Prix/\n    \u251c\u2500\u2500 Formula 1 - S01E01 - Free Practice 1.mkv\n    \u251c\u2500\u2500 Formula 1 - S01E02 - Qualifying.mkv\n    \u251c\u2500\u2500 Formula 1 - S01E03 - Sprint.mkv\n    \u2514\u2500\u2500 Formula 1 - S01E04 - Race.mkv\n</code></pre> <p>Switch <code>link_mode</code> to <code>copy</code> or <code>symlink</code> globally or per sport when working across filesystems or SMB/NFS shares.</p>"},{"location":"operations/#upgrades-backups","title":"Upgrades &amp; Backups","text":"<ul> <li>Docker: pull the latest tag (<code>docker pull ghcr.io/s0len/playbook:latest</code>) and recreate the container. Keep <code>/config</code>, <code>/var/log/playbook</code>, and cache directories mounted so runs resume instantly.</li> <li>Kubernetes: bump the HelmRelease image tag and let Flux reconcile. Cluster CronJobs/Deployments inherit the latest container automatically.</li> <li>Python: upgrade the virtualenv (<code>pip install -U -r requirements.txt</code>).</li> <li>Back up <code>playbook.yaml</code>, notification secrets, and metadata caches if you want to preserve warm-start performance. Everything else is auto-generated.</li> <li>Before promoting a new version, run the container with <code>--dry-run --clear-processed-cache</code> against a staging copy of your downloads to confirm new pattern packs behave as expected.</li> <li>Pin tags (e.g., <code>ghcr.io/s0len/playbook:v1.3.1</code>) in production deployments, then test <code>:latest</code> or <code>develop</code> in a sandbox before rolling forward.</li> <li>Keep a copy of <code>playbook.sample.yaml</code> from the release you are running; diffs against the current sample file quickly highlight breaking config changes.</li> </ul> <p>When you need to troubleshoot, jump to Troubleshooting &amp; FAQ for per-scenario guidance.</p>"},{"location":"recipes/","title":"Recipes &amp; How-tos","text":"<p>These walkthroughs illustrate how to adapt Playbook to new sports, tune regexes, and validate results quickly. Use them as repeatable playbooks for common tasks.</p>"},{"location":"recipes/#quick-reference","title":"Quick Reference","text":"Scenario Recipe Onboard a new sport Extending to New Sports Tweak per-league notifications Customizing Notifications per Sport Build downloader filters Autobrr Filter Walkthrough Vet pattern changes safely Testing Pattern Changes Quickly Validate metadata-driven workflows ISU Figure Skating Example"},{"location":"recipes/#extending-to-new-sports","title":"Extending to New Sports","text":"<ol> <li>Copy <code>config/playbook.sample.yaml</code> and enable the sport by listing the relevant <code>pattern_sets</code> (e.g., <code>formula1</code>, <code>motogp</code>).</li> <li>Set <code>show_slug</code> to reference the show in TVSportsDB, and configure <code>source_globs</code> and <code>source_extensions</code> for your release group.</li> <li>If no template exists yet, copy the closest set from <code>src/playbook/pattern_templates.yaml</code> into your config and adjust the regex/aliases.</li> <li>Run <code>python -m playbook.cli --config playbook.yaml --dry-run --verbose</code> and review both console output and <code>playbook.log</code> for skipped/ignored diagnostics.</li> <li>Iterate on patterns, aliases, and destination templates until every file links where you expect. Consider upstreaming new templates via a PR once battle-tested.</li> </ol>"},{"location":"recipes/#checklist","title":"Checklist","text":"<ul> <li>[ ] Show exists in TVSportsDB with the specified <code>show_slug</code>.</li> <li>[ ] TVSportsDB API reachable from the container/host.</li> <li>[ ] <code>pattern_sets</code> entries spelled exactly as defined in <code>src/playbook/pattern_templates.yaml</code>.</li> <li>[ ] Regex capture groups line up with <code>season_selector</code> and <code>episode_selector</code>.</li> <li>[ ] Notifications configured for the new <code>sport_id</code>.</li> <li>[ ] <code>tests/data/pattern_samples.yaml</code> updated with at least one real release name.</li> </ul>"},{"location":"recipes/#sample-isu-figure-skating-workflow","title":"Sample ISU Figure Skating Workflow","text":"<p>The <code>isu_figure_skating</code> pattern set handles all ISU competitions (Grand Prix, Europeans, Four Continents, Worlds) with <code>show_slug: \"isu-figure-skating-2025-2026\"</code>. Example releases:</p> <ul> <li><code>Figure Skating Grand Prix France 2025 Pairs Short Program 17 10 720pEN50fps ES</code></li> <li><code>ISU Figure Skating Grand Prix China 2025 Ice Dancing Rhythm Dance 18 10</code></li> <li><code>European Figure Skating Championships Sheffield 2026 Men Free Program 17 01</code></li> <li><code>ISU World Figure Skating Championships 2026 Women Free Program 26 03</code></li> </ul> <p>Tips:</p> <ul> <li>The pattern set includes 7 patterns covering all major ISU competitions.</li> <li>When releases omit round numbers, use <code>season_selector.mode: title</code> plus <code>session_aliases</code> to map <code>Rhythm Dance</code>/<code>Free Program</code> onto canonical metadata.</li> <li>Add <code>destination.filename_template</code> overrides if you need federation-specific folder structures (<code>{season_round:02} {season_title}</code> works well for multi-stop tours).</li> <li>Keep <code>allow_unmatched: true</code> during onboarding to reduce noise while you iterate on regexes.</li> </ul>"},{"location":"recipes/#customizing-notifications-per-sport","title":"Customizing Notifications per Sport","text":"<p>Use <code>notifications.mentions</code> to ping only the subscribers who care about a specific league:</p> <pre><code>notifications:\n  mentions:\n    formula1: \"&lt;@&amp;222333444555666777&gt;\"\n    premier_league: \"&lt;@&amp;123456789012345678&gt;\"\n    default: \"@here\"\n</code></pre> <p>Recipes:</p> <ul> <li>Override mentions per target to opt certain Discord webhooks into extra pings.</li> <li>Mix <code>autoscan</code> targets with Discord webhooks so watchers get immediate scans plus human-friendly summaries.</li> <li>Gate daily batch posts through <code>notifications.batch_daily</code> + <code>notifications.flush_time</code> (e.g., set <code>flush_time: \"06:00\"</code> so overnight events roll into the previous \u201cday\u201d).</li> </ul>"},{"location":"recipes/#autobrr-filter-walkthrough","title":"Autobrr Filter Walkthrough","text":"<p>Autobrr filters keep Playbook\u2019s <code>SOURCE_DIR</code> populated with trusted releases. Start from this template:</p> <p><code>--8&lt;-- \"snippets/autobrr-filter.md\"</code></p> <p>How to adapt it:</p> <ol> <li>Update <code>match_releases</code> with sport/year/resolution-specific regexes.</li> <li>Use <code>required_words</code> / <code>excluded_words</code> for release-group whitelisting.</li> <li>Set <code>actions</code> to push directly into qBittorrent/Deluge with the correct category, or trigger custom scripts/webhooks.</li> <li>Run Autobrr\u2019s regex tester before enabling the filter, then monitor the \u201cHistory\u201d tab to confirm matches.</li> </ol> <p>Pair each Autobrr category with a dedicated Playbook sport (or variant) so <code>source_globs</code> stay simple.</p>"},{"location":"recipes/#testing-pattern-changes-quickly","title":"Testing Pattern Changes Quickly","text":"<ul> <li>Edit <code>tests/data/pattern_samples.yaml</code> with real release names and run <code>pytest tests/test_pattern_samples.py</code>.</li> <li>Combine <code>--dry-run</code> with <code>VERBOSE=true</code> to see every capture group and template rendered in the console.</li> <li>Use <code>--clear-processed-cache</code> when you need to reprocess the same files repeatedly during regex tuning.</li> <li>Add <code>--trace-matches</code> to dump match JSON artifacts into <code>cache_dir/traces</code>, then inspect them with your favorite JSON viewer.</li> </ul>"},{"location":"recipes/#validation-runbook","title":"Validation Runbook","text":"<ol> <li><code>python -m playbook.cli validate-config --config playbook.yaml --diff-sample --show-trace</code>.</li> <li><code>python -m playbook.cli --config playbook.yaml --dry-run --verbose --trace-matches</code>.</li> <li>Inspect <code>playbook.log</code> for warnings, unmatched files, or Kometa/Autoscan issues.</li> <li>Tail notifications (Discord/Slack) to ensure new sports trigger the right channels.</li> <li>Promote changes to a staging environment (or run the Docker image with bind mounts) before rolling to production.</li> </ol> <p>Have a favorite workflow that should live here? Open an issue or PR and drop it under <code>docs/recipes.md</code>.</p>"},{"location":"security/","title":"Security &amp; Vulnerability Scanning","text":"<p>This project uses automated vulnerability scanning to detect security issues in Python dependencies and Docker images before they reach production. The security pipeline runs on every pull request, push to develop/main, and weekly on a schedule.</p>"},{"location":"security/#overview","title":"Overview","text":"<p>The security scanning setup consists of three complementary tools:</p> Tool Purpose Scan Target Trigger pip-audit Python dependency CVE scanning <code>requirements.txt</code>, <code>requirements-dev.txt</code> PRs, pushes, weekly, manual Trivy Container vulnerability scanning Docker images (OS + app dependencies) PRs, pushes, weekly, manual Dependabot Automated dependency updates Python packages, GitHub Actions Weekly on Mondays <p>All scans run in the <code>.github/workflows/security-scan.yml</code> workflow and are integrated into the build pipeline. If HIGH or CRITICAL vulnerabilities are detected, the build is blocked from proceeding.</p>"},{"location":"security/#python-dependency-scanning-pip-audit","title":"Python Dependency Scanning (pip-audit)","text":"<p>pip-audit checks Python packages against the PyPI Advisory Database for known CVEs.</p>"},{"location":"security/#what-it-scans","title":"What it scans","text":"<ul> <li>Production dependencies in <code>requirements.txt</code>:</li> <li>PyYAML</li> <li>jsonschema</li> <li>requests</li> <li>python-dateutil</li> <li>rich</li> <li>tenacity</li> <li>rapidfuzz</li> <li>watchdog</li> <li>kubernetes</li> <li>Development dependencies in <code>requirements-dev.txt</code>:</li> <li>pytest</li> <li>mkdocs-material</li> <li>pip-audit itself</li> </ul>"},{"location":"security/#how-it-works","title":"How it works","text":"<p>The workflow runs pip-audit on both dependency files:</p> <pre><code>pip-audit -r requirements.txt --desc\npip-audit -r requirements-dev.txt --desc\n</code></pre> <p>The <code>--desc</code> flag provides detailed descriptions of vulnerabilities, including: - CVE identifier (e.g., CVE-2024-1234) - Affected package and version - Vulnerability description - Fixed version (if available) - Severity level</p>"},{"location":"security/#running-locally","title":"Running locally","text":"<p>Install pip-audit from dev dependencies:</p> <pre><code>pip install -r requirements-dev.txt\n</code></pre> <p>Scan dependencies:</p> <pre><code># Scan production dependencies\npip-audit -r requirements.txt --desc\n\n# Scan development dependencies\npip-audit -r requirements-dev.txt --desc\n\n# Scan currently installed packages\npip-audit\n</code></pre> <p>Additional useful flags:</p> <pre><code># Output as JSON for automation\npip-audit -r requirements.txt --format json\n\n# Only show vulnerabilities (skip informational messages)\npip-audit -r requirements.txt --desc --strict\n\n# Generate a detailed report\npip-audit -r requirements.txt --desc --output audit-report.txt\n</code></pre>"},{"location":"security/#docker-image-scanning-trivy","title":"Docker Image Scanning (Trivy)","text":"<p>Trivy scans the built Docker image for vulnerabilities in: - Base OS packages (from <code>python:3.12-slim</code>) - Python packages installed in the container - Known malware or misconfigurations</p>"},{"location":"security/#severity-levels","title":"Severity levels","text":"<p>Trivy reports vulnerabilities at multiple severity levels:</p> Severity Description CI Action CRITICAL Exploitable vulnerabilities requiring immediate action Blocks build HIGH Serious vulnerabilities that should be addressed quickly Blocks build MEDIUM Moderate vulnerabilities to address in normal workflow Reported only LOW Minor issues or edge cases Reported only UNKNOWN Severity not yet determined Reported only <p>The CI workflow is configured to fail on HIGH or CRITICAL vulnerabilities only, balancing security with practicality.</p>"},{"location":"security/#how-it-works_1","title":"How it works","text":"<ol> <li> <p>The workflow builds the Docker image locally:    <pre><code>docker build -t playbook:scan .\n</code></pre></p> </li> <li> <p>Trivy scans the image for HIGH and CRITICAL vulnerabilities:    <pre><code>trivy image --severity HIGH,CRITICAL playbook:scan\n</code></pre></p> </li> <li> <p>Results are uploaded to GitHub Security tab (SARIF format) and displayed in the workflow logs (table format).</p> </li> </ol>"},{"location":"security/#running-locally_1","title":"Running locally","text":"<p>Scan the Docker image locally:</p> <pre><code># Build the image\ndocker build -t playbook:dev .\n\n# Scan for all vulnerabilities\ntrivy image playbook:dev\n\n# Scan for HIGH and CRITICAL only (matches CI)\ntrivy image --severity HIGH,CRITICAL playbook:dev\n\n# Generate detailed report\ntrivy image --format json --output image-scan.json playbook:dev\n\n# Scan and ignore unfixed vulnerabilities\ntrivy image --ignore-unfixed playbook:dev\n</code></pre> <p>Additional Trivy scanning capabilities:</p> <pre><code># Scan the Dockerfile for best practice violations\ntrivy config Dockerfile\n\n# Scan local filesystem for secrets\ntrivy fs .\n\n# Scan a specific Python package\ntrivy rootfs --pkg-types library /path/to/venv\n</code></pre>"},{"location":"security/#dependabot-automated-updates","title":"Dependabot Automated Updates","text":"<p>Dependabot automatically creates pull requests to update dependencies when: - New versions are released - Security advisories are published - Dependencies become outdated</p>"},{"location":"security/#configuration","title":"Configuration","text":"<p>Dependabot is configured in <code>.github/dependabot.yml</code>:</p> <ul> <li>Schedule: Weekly on Mondays at 9:00 AM UTC (aligned with security scans)</li> <li>Pull request limit: 5 open PRs per ecosystem</li> <li>Ecosystems monitored:</li> <li><code>pip</code> \u2013 Python packages in <code>requirements.txt</code> and <code>requirements-dev.txt</code></li> <li><code>github-actions</code> \u2013 GitHub Actions versions in <code>.github/workflows/</code></li> <li>Labels: PRs are tagged with <code>dependencies</code>, <code>python</code>/<code>github-actions</code>, and <code>security</code></li> <li>Commit prefixes: <code>deps</code> for Python, <code>ci</code> for GitHub Actions</li> </ul>"},{"location":"security/#reviewing-dependabot-prs","title":"Reviewing Dependabot PRs","text":"<p>When Dependabot opens a PR:</p> <ol> <li>Check the changelog \u2013 Review what changed in the new version</li> <li>Review security advisories \u2013 If the PR addresses a CVE, prioritize it</li> <li>Run tests locally \u2013 Ensure the update doesn't break functionality:    <pre><code>git fetch origin\ngit checkout dependabot/pip/package-name-1.2.3\npip install -r requirements.txt\npytest\n</code></pre></li> <li>Check CI results \u2013 Security scans and tests run automatically</li> <li>Merge if green \u2013 Dependabot PRs that pass all checks are safe to merge</li> </ol>"},{"location":"security/#managing-dependabot","title":"Managing Dependabot","text":"<pre><code># Rebase a stale Dependabot PR\n@dependabot rebase\n\n# Recreate a PR\n@dependabot recreate\n\n# Merge a PR\n@dependabot merge\n\n# Ignore a specific version\n@dependabot ignore this version\n\n# Ignore a major version\n@dependabot ignore this major version\n</code></pre>"},{"location":"security/#interpreting-scan-results","title":"Interpreting Scan Results","text":""},{"location":"security/#github-security-tab","title":"GitHub Security Tab","text":"<p>All Trivy scan results appear in the repository's Security \u2192 Code scanning tab. This provides: - Centralized view of all vulnerabilities - Filtering by severity, state, and branch - Links to CVE details and remediation guidance - Historical tracking of when issues were introduced/fixed</p>"},{"location":"security/#workflow-logs","title":"Workflow Logs","text":"<p>Security scan results appear in the GitHub Actions workflow logs:</p> <p>pip-audit output example: <pre><code>Found 2 known vulnerabilities in 1 package\nName    Version ID             Fix Versions\n------- ------- -------------- ------------\nurllib3 1.26.5  GHSA-q2q7-5pp4 1.26.18,2.0.7\nurllib3 1.26.5  PYSEC-2023-74  1.26.16\n</code></pre></p> <p>Trivy output example: <pre><code>playbook:scan (debian 12.4)\nTotal: 5 (HIGH: 3, CRITICAL: 2)\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Library    \u2502 Vulnerability  \u2502 Severity \u2502 Installed Version \u2502 Fixed Version \u2502     Title       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openssl      \u2502 CVE-2024-1234  \u2502 CRITICAL \u2502 3.0.11-1          \u2502 3.0.13-1      \u2502 Buffer overflow \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"security/#handling-reported-vulnerabilities","title":"Handling Reported Vulnerabilities","text":"<p>When a vulnerability is detected:</p>"},{"location":"security/#1-assess-severity-and-impact","title":"1. Assess severity and impact","text":"<ul> <li>CRITICAL/HIGH in production dependencies: Address immediately</li> <li>CRITICAL/HIGH in dev dependencies: Address in next sprint</li> <li>MEDIUM/LOW: Schedule for routine maintenance</li> <li>Unfixed vulnerabilities: Evaluate workarounds or risk acceptance</li> </ul>"},{"location":"security/#2-check-for-available-fixes","title":"2. Check for available fixes","text":"<pre><code># For Python packages, check PyPI for newer versions\npip index versions &lt;package-name&gt;\n\n# Check the CVE database for fix details\npip-audit -r requirements.txt --desc\n</code></pre>"},{"location":"security/#3-update-the-dependency","title":"3. Update the dependency","text":"<p>Option A: Direct update (preferred)</p> <ol> <li> <p>Update <code>requirements.txt</code>:    <pre><code>-requests==2.31.0\n+requests==2.32.5\n</code></pre></p> </li> <li> <p>Test locally:    <pre><code>pip install -r requirements.txt\npytest\npython -m playbook.cli --dry-run --config config/playbook.sample.yaml\n</code></pre></p> </li> <li> <p>Commit and push:    <pre><code>git add requirements.txt\ngit commit -m \"deps: Update requests to 2.32.5 to fix CVE-2024-XXXX\"\ngit push\n</code></pre></p> </li> </ol> <p>Option B: Wait for Dependabot</p> <p>If the fix was recently released, Dependabot will create a PR within a week. Monitor the Security tab for updates.</p> <p>Option C: Pin to a safe version</p> <p>If the latest version introduces breaking changes:</p> <ol> <li>Pin to the newest safe version that fixes the CVE</li> <li>Create a tracking issue for upgrading to the latest version</li> <li>Document the decision in the commit message</li> </ol>"},{"location":"security/#4-verify-the-fix","title":"4. Verify the fix","text":"<p>After updating:</p> <pre><code># Verify Python dependencies are clean\npip-audit -r requirements.txt\n\n# Rebuild and scan Docker image\ndocker build -t playbook:dev .\ntrivy image --severity HIGH,CRITICAL playbook:dev\n</code></pre> <p>The CI workflow will also re-scan on the next push.</p>"},{"location":"security/#5-handle-unfixed-vulnerabilities","title":"5. Handle unfixed vulnerabilities","text":"<p>If no fix is available:</p> <ol> <li>Assess exploitability \u2013 Is this vulnerability exploitable in Playbook's context?</li> <li>Check for workarounds \u2013 Can you mitigate the risk through configuration?</li> <li>File an upstream issue \u2013 Alert the package maintainers</li> <li>Consider alternatives \u2013 Can you switch to a different package?</li> <li>Document the risk \u2013 Add a comment in <code>requirements.txt</code> explaining why the vulnerable version is pinned</li> <li>Suppress false positives (last resort):    <pre><code># Create .trivyignore file for Trivy\necho \"CVE-2024-XXXX\" &gt;&gt; .trivyignore\n\n# Use pip-audit vulnerability ignore file\necho \"GHSA-XXXX-XXXX-XXXX\" &gt;&gt; .pip-audit-ignore.json\n</code></pre></li> </ol>"},{"location":"security/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"security/#security-scan-workflow-triggers","title":"Security scan workflow triggers","text":"<p>The security scan workflow (<code>.github/workflows/security-scan.yml</code>) runs on:</p> <ol> <li>Pull requests \u2013 Gates merges via required status checks</li> <li>Pushes to <code>develop</code>/<code>main</code> \u2013 Catches issues before deployment</li> <li>Weekly schedule \u2013 Monday at 9:00 AM UTC to catch newly disclosed CVEs</li> <li>Manual trigger \u2013 Via Actions tab for ad-hoc audits</li> <li>File changes \u2013 Only runs when <code>requirements*.txt</code>, <code>Dockerfile</code>, or the workflow itself changes</li> </ol>"},{"location":"security/#build-pipeline-integration","title":"Build pipeline integration","text":"<p>The build-and-push workflow (<code>.github/workflows/build-and-push.yml</code>) includes security scans as prerequisites:</p> <pre><code>jobs:\n  python-dependency-scan:\n    # ... runs pip-audit ...\n\n  docker-image-scan:\n    # ... runs Trivy ...\n\n  build-and-push:\n    needs: [python-dependency-scan, docker-image-scan]\n    # ... builds and pushes Docker image ...\n</code></pre> <p>This ensures: - Security scans run before building Docker images - Build is blocked if HIGH or CRITICAL vulnerabilities are found - No vulnerable images are pushed to the registry - Failed scans prevent deployment to production</p>"},{"location":"security/#setting-up-required-status-checks","title":"Setting up required status checks","text":"<p>To enforce security scans on pull requests:</p> <ol> <li>Go to Settings \u2192 Branches \u2192 Branch protection rules</li> <li>Edit the rule for <code>develop</code> and <code>main</code></li> <li>Enable Require status checks to pass before merging</li> <li>Search for and add:</li> <li><code>python-dependency-scan</code></li> <li><code>docker-image-scan</code></li> <li>Save changes</li> </ol> <p>Now PRs cannot be merged if security scans fail.</p>"},{"location":"security/#best-practices","title":"Best Practices","text":""},{"location":"security/#for-developers","title":"For developers","text":"<ul> <li>\u2705 Run <code>pip-audit</code> locally before committing dependency updates</li> <li>\u2705 Keep dependencies pinned to specific versions in <code>requirements.txt</code></li> <li>\u2705 Review Dependabot PRs promptly, especially security updates</li> <li>\u2705 Test locally after updating dependencies</li> <li>\u2705 Document any risk acceptance decisions in commit messages or issues</li> </ul>"},{"location":"security/#for-maintainers","title":"For maintainers","text":"<ul> <li>\u2705 Monitor the GitHub Security tab weekly</li> <li>\u2705 Set security scans as required status checks on protected branches</li> <li>\u2705 Triage vulnerability reports within 1 business day</li> <li>\u2705 Prioritize CRITICAL/HIGH fixes in current sprint</li> <li>\u2705 Keep base Docker images updated (<code>python:3.12-slim</code>)</li> <li>\u2705 Review <code>.trivyignore</code> and <code>.pip-audit-ignore.json</code> quarterly</li> </ul>"},{"location":"security/#for-cicd","title":"For CI/CD","text":"<ul> <li>\u2705 Run security scans before building artifacts</li> <li>\u2705 Fail fast on CRITICAL/HIGH vulnerabilities</li> <li>\u2705 Upload SARIF reports to GitHub Security tab for tracking</li> <li>\u2705 Schedule regular scans to catch new CVEs</li> <li>\u2705 Monitor workflow failures and fix them promptly</li> </ul>"},{"location":"security/#troubleshooting","title":"Troubleshooting","text":""},{"location":"security/#scan-failing-due-to-network-issues","title":"Scan failing due to network issues","text":"<pre><code># pip-audit can't reach PyPI Advisory Database\nError: Failed to download vulnerability database\n\n# Solution: Retry the workflow or check network connectivity\n</code></pre>"},{"location":"security/#false-positives-in-trivy","title":"False positives in Trivy","text":"<p>Some vulnerabilities may not apply to Playbook's use case:</p> <pre><code># Create .trivyignore file\ncat &gt; .trivyignore &lt;&lt;EOF\n# Not exploitable: Playbook doesn't use affected code path\nCVE-2024-XXXX\nEOF\n</code></pre> <p>Document the reason for ignoring in comments.</p>"},{"location":"security/#dependabot-prs-failing-tests","title":"Dependabot PRs failing tests","text":"<p>If a Dependabot PR breaks tests:</p> <ol> <li>Comment <code>@dependabot ignore this major version</code> to skip major updates</li> <li>Investigate breaking changes in the package changelog</li> <li>Update Playbook code to handle API changes</li> <li>Manually create a PR with the fix</li> </ol>"},{"location":"security/#security-scan-taking-too-long","title":"Security scan taking too long","text":"<p>Large Docker images can slow down Trivy scans. To optimize:</p> <pre><code># Use multi-stage builds to reduce final image size\nFROM python:3.12-slim AS builder\n# ... build steps ...\n\nFROM python:3.12-slim\nCOPY --from=builder /app /app\n</code></pre>"},{"location":"security/#additional-resources","title":"Additional Resources","text":"<ul> <li>pip-audit documentation</li> <li>Trivy documentation</li> <li>Dependabot documentation</li> <li>GitHub Security tab</li> <li>PyPI Advisory Database</li> <li>CVE database</li> </ul> <p>Need help? Check Troubleshooting &amp; FAQ or open a GitHub issue.</p>"},{"location":"troubleshooting/","title":"Troubleshooting &amp; FAQ","text":"<p>Run into snags? Start with the quick triage grid below, then follow the deeper diagnostics checklist to narrow things down.</p>"},{"location":"troubleshooting/#quick-triage","title":"Quick Triage","text":"Symptom Likely cause Immediate next step CLI finishes instantly, nothing processed <code>source_dir</code> not mounted/readable or <code>source_globs</code> too strict Run <code>python -m playbook.cli --config ... --dry-run --verbose</code> and inspect <code>playbook.log</code> for \"skipped\" entries Metadata looks stale / missing events Cached API responses past TTL or wrong <code>show_slug</code> Delete <code>CACHE_DIR/tvsportsdb/*</code> or lower <code>tvsportsdb.ttl_hours</code>, then rerun with <code>--dry-run</code> Show not found in TVSportsDB Invalid <code>show_slug</code> or show doesn't exist yet Verify the slug exists in TVSportsDB; check API connectivity Hardlinks fail / files missing Cross-filesystem moves or SMB/NFS target Set <code>link_mode: copy</code> (global or per sport) and ensure destination mount permissions Kometa/Autoscan never triggers <code>settings.kometa_trigger.enabled</code> false or Autoscan rewrites wrong Run <code>python -m playbook.cli kometa-trigger ...</code> to test; review notification targets for correct rewrites Watcher never fires <code>watchdog</code> missing or paths misconfigured Check container logs for \"watchdog unavailable\", confirm <code>file_watcher.paths</code> exist, or fall back to batch mode"},{"location":"troubleshooting/#diagnostics-workflow","title":"Diagnostics Workflow","text":"<ol> <li>Validate config: <code>python -m playbook.cli validate-config --config playbook.yaml --diff-sample --show-trace</code>. Catch schema errors before wasting time elsewhere.</li> <li>Instrumented dry-run: <code>python -m playbook.cli --config playbook.yaml --dry-run --verbose --trace-matches</code>. This produces console DEBUG output, persistent logs, and per-file trace JSON (under <code>cache_dir/traces</code>).</li> <li>Review logs: Inspect <code>playbook.log</code> for warnings (missing metadata, Kometa trigger failures, Autoscan errors). The file rotates to <code>playbook.log.previous</code> every run\u2014keep both when filing issues.</li> <li>Reset processed cache (optional): <code>--clear-processed-cache</code> (or <code>CLEAR_PROCESSED_CACHE=true</code>) forces Playbook to treat every file as new\u2014useful when you want to re-run test datasets.</li> <li>Isolate integrations:</li> <li>Kometa: <code>python -m playbook.cli kometa-trigger --config ... --mode docker</code></li> <li>Autoscan: temporarily disable other targets so logs only contain Autoscan responses.</li> <li>Notifications: point Discord/Slack webhooks at a test channel until you confirm formatting.</li> </ol>"},{"location":"troubleshooting/#understanding-validation-output","title":"Understanding Validation Output","text":"<p>The <code>validate-config</code> command provides enhanced error reporting with grouped issues, line numbers, and actionable fix suggestions to help you quickly identify and resolve configuration problems.</p>"},{"location":"troubleshooting/#output-format","title":"Output Format","text":"<p>Validation errors are organized by configuration section with visual panels:</p> <pre><code>Validation Errors: 3 error(s) detected\n\n\u256d\u2500 Settings \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u2192 Notification Settings                            \u2502\n\u2502  L6    settings.notifications.flush_time           \u2502\n\u2502        Invalid time format: 'invalid_number'       \u2502\n\u2502        (flush-time)                                \u2502\n\u2502        \ud83d\udca1 Use HH:MM or HH:MM:SS format (e.g.,      \u2502\n\u2502           '23:30' or '23:30:00')                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u256d\u2500 Sport #1 (demo) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  L10   sports[0].show_slug                         \u2502\n\u2502        Sport must define show_slug or variants     \u2502\n\u2502        with show_slug                              \u2502\n\u2502        (show-slug-missing)                         \u2502\n\u2502        \ud83d\udca1 Add 'show_slug' field referencing a      \u2502\n\u2502           show in TVSportsDB                    \u2502\n\u2502                                                    \u2502\n\u2502  L11   sports[0].id                                \u2502\n\u2502        Duplicate sport ID: 'demo'                  \u2502\n\u2502        (duplicate-id)                              \u2502\n\u2502        \ud83d\udca1 Change the 'id' field to a unique value  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"troubleshooting/#key-elements","title":"Key Elements","text":"<ul> <li>Grouped sections: Errors are grouped by configuration area (Settings, Sport #1, Pattern Sets, etc.) making it easy to focus on one part of your config at a time</li> <li>Line numbers: Each error shows <code>L&lt;number&gt;</code> indicating the exact line in your YAML file where the issue occurs</li> <li>Full path: The complete path to the problematic field (e.g., <code>sports[0].show_slug</code>)</li> <li>Error codes: Technical error identifiers in parentheses (e.g., <code>(flush-time)</code>, <code>(show-slug-missing)</code>) useful for searching documentation</li> <li>Fix suggestions: Actionable guidance marked with \ud83d\udca1 that explains how to correct the issue</li> </ul>"},{"location":"troubleshooting/#interpreting-fix-suggestions","title":"Interpreting Fix Suggestions","text":"<p>Fix suggestions are context-aware and provide specific guidance based on the error:</p> <ul> <li> <p>Schema errors: Show expected type/format and provide valid examples   <pre><code>\ud83d\udca1 Expected type: string, but got: integer\n   Wrap the value in quotes to make it a string\n</code></pre></p> </li> <li> <p>Format errors: Explain the required format with concrete examples   <pre><code>\ud83d\udca1 Use HH:MM or HH:MM:SS format (e.g., '23:30' or '23:30:00')\n</code></pre></p> </li> <li> <p>Missing fields: Tell you exactly which required field to add   <pre><code>\ud83d\udca1 Add 'show_slug' field referencing a show in TVSportsDB\n</code></pre></p> </li> <li> <p>Invalid values: Suggest valid alternatives or corrections   <pre><code>\ud83d\udca1 Change the 'id' field to a unique value. Current duplicates: demo\n</code></pre></p> </li> </ul>"},{"location":"troubleshooting/#command-options","title":"Command Options","text":"<p>Hide fix suggestions for cleaner output: <pre><code>python -m playbook.cli validate-config --config playbook.yaml --no-suggestions\n</code></pre></p> <p>This shows only the errors and line numbers without the \ud83d\udca1 guidance\u2014useful for quick scans or when you're already familiar with the fixes.</p> <p>Full validation with all features: <pre><code>python -m playbook.cli validate-config --config playbook.yaml --diff-sample --show-trace\n</code></pre></p> <p>Includes diff samples and trace information for deep debugging alongside the grouped error output.</p>"},{"location":"troubleshooting/#command-cheat-sheet","title":"Command Cheat Sheet","text":"<pre><code># Validate config with diff\npython -m playbook.cli validate-config --config /config/playbook.yaml --diff-sample --show-trace\n\n# Verbose dry-run with match traces\npython -m playbook.cli --config /config/playbook.yaml --dry-run --verbose --trace-matches\n\n# Force watcher mode off (single pass)\npython -m playbook.cli --config /config/playbook.yaml --no-watch\n\n# Manual Kometa trigger diagnostics\npython -m playbook.cli kometa-trigger --config /config/playbook.yaml --mode docker --verbose\n</code></pre>"},{"location":"troubleshooting/#cache-filesystem-hygiene","title":"Cache &amp; Filesystem Hygiene","text":"<ul> <li>Safe to delete: <code>CACHE_DIR/tvsportsdb/*</code> (forces API refetch), <code>CACHE_DIR/processed_cache.json</code>, <code>LOG_DIR/*.previous</code></li> <li>Keep persistent: <code>playbook.yaml</code>, notification secrets, destination directory (Plex expects consistent paths), Kometa configs</li> <li>Hardlink tips:   Use <code>link_mode: copy</code> or <code>symlink</code> when crossing filesystems. On SMB/NFS mounts, confirm the server allows hardlinks or skip straight to copy mode.</li> </ul>"},{"location":"troubleshooting/#integration-troubleshooting","title":"Integration Troubleshooting","text":""},{"location":"troubleshooting/#kometa","title":"Kometa","text":"<ul> <li><code>Kometa trigger is disabled</code> \u2192 enable <code>settings.kometa_trigger.enabled</code> or pass <code>--mode docker/kubernetes</code> to the CLI.</li> <li><code>Kometa docker trigger requires access to the Docker socket</code> \u2192 mount <code>/var/run/docker.sock</code> plus the Docker client binaries into the Playbook container.</li> <li>Kubernetes jobs stuck in <code>Active</code> \u2192 <code>kubectl delete job -l trigger=playbook</code> to clear them, then rerun.</li> <li>Use <code>kometa_trigger.docker.exec_command</code> to match whatever entrypoint your Compose stack uses; Playbook appends <code>--run-libraries</code>/<code>--config</code> automatically.</li> </ul>"},{"location":"troubleshooting/#autoscan-plex-refreshes","title":"Autoscan / Plex refreshes","text":"<ul> <li>404 errors \u2192 Confirm Autoscan URL is reachable from Playbook's network namespace; check Docker network configuration and port mappings.</li> <li>SSL/TLS certificate errors \u2192 See Certificate Configuration below for proper solutions. Do not disable SSL verification unless you understand the security risks.</li> <li>Nothing happens after linking files \u2192 Verify <code>rewrite</code> entries translate Playbook's destination into what Autoscan/Plex can see (inside Docker, paths often differ).</li> <li>Plex scans stale metadata \u2192 Pair Autoscan with Kometa triggers; Plex rescans directories immediately while Kometa updates titles/artwork.</li> </ul>"},{"location":"troubleshooting/#certificate-configuration","title":"Certificate Configuration","text":"<p>If you encounter SSL/TLS certificate verification errors when connecting to Autoscan, do not immediately disable verification. Instead, use one of these proper solutions:</p> <ol> <li>Use a CA-signed certificate (recommended for production):</li> <li>Obtain a certificate from Let's Encrypt or another trusted Certificate Authority</li> <li>Configure your Autoscan server to use the CA-signed certificate</li> <li> <p>No Playbook configuration changes needed\u2014verification will work automatically</p> </li> <li> <p>Add self-signed certificate to trust store (for development/internal networks):    <pre><code># Copy your self-signed certificate into the container\ndocker cp autoscan-cert.crt playbook:/usr/local/share/ca-certificates/\n\n# Update the certificate trust store\ndocker exec playbook update-ca-certificates\n\n# Restart Playbook to pick up the new certificate\ndocker restart playbook\n</code></pre></p> </li> <li> <p>Disable verification (\u26a0\ufe0f NOT RECOMMENDED):</p> </li> <li>Only use this as a last resort in isolated development/testing environments</li> <li>Setting <code>verify_ssl: false</code> exposes your system to man-in-the-middle (MITM) attacks</li> <li>NEVER use this in production or on networks you don't fully control</li> <li>See the Autoscan integration docs for detailed security warnings</li> </ol> <p>The proper certificate configuration approach protects your system from MITM attacks while eliminating SSL errors. If you're still having issues after trying the above, review your certificate's Common Name (CN) or Subject Alternative Names (SAN) to ensure they match the hostname in your Autoscan URL.</p>"},{"location":"troubleshooting/#autobrr-downloader-feeds","title":"Autobrr / Downloader feeds","text":"<ul> <li>Filters never match \u2192 use Autobrr\u2019s regex tester and ensure <code>match_releases</code> are case-insensitive (<code>(?i)</code>), include the season/year, and restrict to trusted encoders.</li> <li>Wrong category/path \u2192 align Autobrr\u2019s action category (e.g., <code>sports/f1</code>) with Playbook\u2019s <code>source_globs</code> so new files land where pattern sets expect them.</li> <li>Duplicate grabs \u2192 leverage Autobrr <code>priority</code> and <code>excluded_words</code> to prefer one release group per sport.</li> </ul>"},{"location":"troubleshooting/#faq","title":"FAQ","text":"<p>Can I run multiple configs? Yes\u2014launch separate containers/CLI processes with distinct <code>CONFIG_PATH</code>, <code>SOURCE_DIR</code>, and <code>DESTINATION_DIR</code>. Avoid pointing two configs at the same destination unless you coordinate notifications carefully.</p> <p>Does Playbook delete or move downloads? No. Default is hardlinking. Switch to copy/symlink when cross-filesystem moves or NAS targets require it.</p> <p>How do I add a brand-new sport? Follow the steps in Recipes &amp; How-tos, add real sample filenames to <code>tests/data/pattern_samples.yaml</code>, and consider upstreaming new pattern sets.</p> <p>Watcher vs batch\u2014what should I use? Use batch (cron/systemd) for infrequent reorganizations, watcher mode for continuous ingest. Even with watcher mode, schedule a nightly batch run as a safety net if your platform occasionally drops filesystem events.</p> <p>Where should I ask for help? Open an issue with: - Playbook version (container tag or commit SHA) - Relevant excerpts from <code>playbook.log</code> - Redacted <code>playbook.yaml</code> (remove secrets) - Example filenames that failed to match</p> <p>Still blocked? Start a discussion or open an issue and we\u2019ll dig in. The more context (logs, configs, sample names) you provide, the faster we can help.</p>"},{"location":"assets/","title":"Assets","text":"<p>Drop diagrams, screenshots, and other binary/static files here. Reference them from Markdown via relative links, e.g. <code>![Kometa trigger flow](assets/kometa-flow.png)</code>. Keep source files (draw.io, Excalidraw) alongside the exported image when possible so future contributors can tweak them.</p>"},{"location":"snippets/","title":"Snippets","text":"<p>Reusable Markdown/YAML/CLI fragments live here. Use the Material <code>--8&lt;-- \"snippets/&lt;file&gt;.md\"</code> include syntax to pull them into multiple pages without duplicating content. Keep snippets short and focused (e.g., Docker compose blocks, recurring warning callouts).</p>"},{"location":"snippets/autobrr-filter/","title":"Autobrr filter","text":"<pre><code>[[filters]]\nname = \"F1 1080p MWR\"\nenabled = true\npriority = 10\nmatch_releases = [\n  \"(?i)(F1|Formula.1).*2025.*Round\\\\d+.*(FP\\\\d?|Qualifying|Sprint|Race).*1080p.*MWR\"\n]\ninclude = [\"f1-seasonal\"]\nactions = [\n  { type = \"push\", target = \"qbittorrent\", category = \"sports/f1\" },\n  { type = \"exec\", command = \"/usr/local/bin/notify-new-torrent.sh\" }\n]\nrequired_words = [\"mkv\", \"x264\"]\nexcluded_words = [\"480p\", \"cam\"]\nmax_size = \"15 GB\"\n</code></pre>"},{"location":"snippets/cron-batch-job/","title":"Cron batch job","text":"<pre><code>0 * * * * /usr/bin/docker run --rm \\\n  --name playbook-hourly \\\n  -e SOURCE_DIR=\"/downloads\" \\\n  -e DESTINATION_DIR=\"/library\" \\\n  -e CACHE_DIR=\"/cache\" \\\n  -e LOG_DIR=\"/logs/playbook\" \\\n  -v /srv/playbook/config:/config \\\n  -v /srv/downloads:/data/source \\\n  -v /srv/library:/data/destination \\\n  -v /srv/cache:/var/cache/playbook \\\n  -v /srv/logs:/var/log/playbook \\\n  ghcr.io/s0len/playbook:latest --dry-run\n</code></pre>"},{"location":"snippets/notifications-autoscan/","title":"Notifications autoscan","text":"<pre><code>notifications:\n  targets:\n    - type: autoscan\n      url: http://autoscan:3030\n      trigger: manual            # optional; defaults to /triggers/manual\n      username: ${AUTOSCAN_USER:-}\n      password: ${AUTOSCAN_PASS:-}\n      rewrite:\n        - from: ${DESTINATION_DIR:-/data/destination}\n          to: /mnt/unionfs/Media\n        - from: /data/destination\n          to: /Volumes/Media\n      timeout: 10\n      verify_ssl: true                   # \u26a0\ufe0f SECURITY WARNING: Setting false disables SSL/TLS verification and exposes you to MITM attacks - only for development with self-signed certs\n</code></pre>"},{"location":"snippets/systemd-watcher-service/","title":"Systemd watcher service","text":"<pre><code>[Unit]\nDescription=Playbook watcher\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nWorkingDirectory=/opt/playbook\nEnvironmentFile=/etc/playbook.env\nExecStart=/opt/playbook/.venv/bin/python -m playbook.cli --config /opt/playbook/config/playbook.yaml --watch\nRestart=on-failure\nRestartSec=5s\nUser=playbook\nGroup=playbook\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"}]}